{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Downloading tensorflow-2.6.0-cp38-cp38-win_amd64.whl (423.2 MB)\n",
      "Processing c:\\users\\何佳馨\\appdata\\local\\pip\\cache\\wheels\\a0\\16\\9c\\5473df82468f958445479c59e784896fa24f4a5fc024b0f501\\termcolor-1.1.0-py3-none-any.whl\n",
      "Collecting opt-einsum~=3.3.0\n",
      "  Using cached opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "Processing c:\\users\\何佳馨\\appdata\\local\\pip\\cache\\wheels\\5f\\fd\\9e\\b6cf5890494cb8ef0b5eaff72e5d55a70fb56316007d6dfe73\\wrapt-1.12.1-cp38-cp38-win_amd64.whl\n",
      "Collecting flatbuffers~=1.12.0\n",
      "  Using cached flatbuffers-1.12-py2.py3-none-any.whl (15 kB)\n",
      "Collecting astunparse~=1.6.3\n",
      "  Using cached astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Collecting grpcio<2.0,>=1.37.0\n",
      "  Downloading grpcio-1.39.0-cp38-cp38-win_amd64.whl (3.2 MB)\n",
      "Collecting tensorflow-estimator~=2.6\n",
      "  Downloading tensorflow_estimator-2.6.0-py2.py3-none-any.whl (462 kB)\n",
      "Collecting wheel~=0.35\n",
      "  Using cached wheel-0.37.0-py2.py3-none-any.whl (35 kB)\n",
      "Collecting protobuf>=3.9.2\n",
      "  Downloading protobuf-3.17.3-cp38-cp38-win_amd64.whl (909 kB)\n",
      "Collecting google-pasta~=0.2\n",
      "  Using cached google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Collecting absl-py~=0.10\n",
      "  Downloading absl_py-0.13.0-py3-none-any.whl (132 kB)\n",
      "Collecting keras~=2.6\n",
      "  Downloading keras-2.6.0-py2.py3-none-any.whl (1.3 MB)\n",
      "Collecting typing-extensions~=3.7.4\n",
      "  Using cached typing_extensions-3.7.4.3-py3-none-any.whl (22 kB)\n",
      "Collecting clang~=5.0\n",
      "  Downloading clang-5.0.tar.gz (30 kB)\n",
      "Collecting gast==0.4.0\n",
      "  Using cached gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
      "Collecting h5py~=3.1.0\n",
      "  Using cached h5py-3.1.0-cp38-cp38-win_amd64.whl (2.7 MB)\n",
      "Collecting six~=1.15.0\n",
      "  Using cached six-1.15.0-py2.py3-none-any.whl (10 kB)\n",
      "Collecting keras-preprocessing~=1.1.2\n",
      "  Using cached Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
      "Collecting tensorboard~=2.6\n",
      "  Downloading tensorboard-2.6.0-py3-none-any.whl (5.6 MB)\n",
      "Collecting numpy~=1.19.2\n",
      "  Downloading numpy-1.19.5-cp38-cp38-win_amd64.whl (13.3 MB)\n",
      "Collecting tensorboard-plugin-wit>=1.6.0\n",
      "  Using cached tensorboard_plugin_wit-1.8.0-py3-none-any.whl (781 kB)\n",
      "Collecting werkzeug>=0.11.15\n",
      "  Using cached Werkzeug-2.0.1-py3-none-any.whl (288 kB)\n",
      "Collecting setuptools>=41.0.0\n",
      "  Using cached setuptools-57.4.0-py3-none-any.whl (819 kB)\n",
      "Collecting google-auth<2,>=1.6.3\n",
      "  Downloading google_auth-1.35.0-py2.py3-none-any.whl (152 kB)\n",
      "Collecting markdown>=2.6.8\n",
      "  Using cached Markdown-3.3.4-py3-none-any.whl (97 kB)\n",
      "Collecting requests<3,>=2.21.0\n",
      "  Downloading requests-2.26.0-py2.py3-none-any.whl (62 kB)\n",
      "Collecting tensorboard-data-server<0.7.0,>=0.6.0\n",
      "  Using cached tensorboard_data_server-0.6.1-py3-none-any.whl (2.4 kB)\n",
      "Collecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Downloading google_auth_oauthlib-0.4.5-py2.py3-none-any.whl (18 kB)\n",
      "Collecting cachetools<5.0,>=2.0.0\n",
      "  Using cached cachetools-4.2.2-py3-none-any.whl (11 kB)\n",
      "Collecting rsa<5,>=3.1.4; python_version >= \"3.6\"\n",
      "  Using cached rsa-4.7.2-py3-none-any.whl (34 kB)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Using cached pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
      "Collecting certifi>=2017.4.17\n",
      "  Downloading certifi-2021.5.30-py2.py3-none-any.whl (145 kB)\n",
      "Collecting idna<4,>=2.5; python_version >= \"3\"\n",
      "  Downloading idna-3.2-py3-none-any.whl (59 kB)\n",
      "Collecting charset-normalizer~=2.0.0; python_version >= \"3\"\n",
      "  Downloading charset_normalizer-2.0.4-py3-none-any.whl (36 kB)\n",
      "Collecting urllib3<1.27,>=1.21.1\n",
      "  Downloading urllib3-1.26.6-py2.py3-none-any.whl (138 kB)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Using cached requests_oauthlib-1.3.0-py2.py3-none-any.whl (23 kB)\n",
      "Collecting pyasn1>=0.1.3\n",
      "  Using cached pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\n",
      "Collecting oauthlib>=3.0.0\n",
      "  Using cached oauthlib-3.1.1-py2.py3-none-any.whl (146 kB)\n",
      "Building wheels for collected packages: clang\n",
      "  Building wheel for clang (setup.py): started\n",
      "  Building wheel for clang (setup.py): finished with status 'done'\n",
      "  Created wheel for clang: filename=clang-5.0-py3-none-any.whl size=30710 sha256=49f164202a1ca43e73a247cfc4da0508fd1474d1d0d296dab4b5fca040440464\n",
      "  Stored in directory: c:\\users\\何佳馨\\appdata\\local\\pip\\cache\\wheels\\f1\\60\\77\\22b9b5887bd47801796a856f47650d9789c74dc3161a26d608\n",
      "Successfully built clang\n",
      "Installing collected packages: termcolor, numpy, opt-einsum, wrapt, flatbuffers, wheel, six, astunparse, grpcio, tensorflow-estimator, protobuf, google-pasta, absl-py, keras, typing-extensions, clang, gast, h5py, keras-preprocessing, tensorboard-plugin-wit, werkzeug, setuptools, cachetools, pyasn1, rsa, pyasn1-modules, google-auth, markdown, certifi, idna, charset-normalizer, urllib3, requests, tensorboard-data-server, oauthlib, requests-oauthlib, google-auth-oauthlib, tensorboard, tensorflow\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow_datasets'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-9f40bbb0d77d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow_datasets\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtfds\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__version__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow_datasets'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from pprint import pprint\n",
    "from IPython.display import clear_output\n",
    "!pip3 install --ignore-installed --upgrade --ignore-installed tensorflow\n",
    "clear_output()\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow-gpu\n",
      "  Downloading tensorflow_gpu-2.6.0-cp38-cp38-win_amd64.whl (423.3 MB)\n",
      "Requirement already satisfied, skipping upgrade: typing-extensions~=3.7.4 in c:\\amy\\anaconda3\\lib\\site-packages (from tensorflow-gpu) (3.7.4.3)\n",
      "Requirement already satisfied, skipping upgrade: absl-py~=0.10 in c:\\users\\何佳馨\\appdata\\roaming\\python\\python38\\site-packages (from tensorflow-gpu) (0.12.0)\n",
      "Collecting tensorboard~=2.6\n",
      "  Using cached tensorboard-2.6.0-py3-none-any.whl (5.6 MB)\n",
      "Requirement already satisfied, skipping upgrade: astunparse~=1.6.3 in c:\\users\\何佳馨\\appdata\\roaming\\python\\python38\\site-packages (from tensorflow-gpu) (1.6.3)\n",
      "Requirement already satisfied, skipping upgrade: keras-preprocessing~=1.1.2 in c:\\users\\何佳馨\\appdata\\roaming\\python\\python38\\site-packages (from tensorflow-gpu) (1.1.2)\n",
      "Requirement already satisfied, skipping upgrade: opt-einsum~=3.3.0 in c:\\users\\何佳馨\\appdata\\roaming\\python\\python38\\site-packages (from tensorflow-gpu) (3.3.0)\n",
      "Processing c:\\users\\何佳馨\\appdata\\local\\pip\\cache\\wheels\\f1\\60\\77\\22b9b5887bd47801796a856f47650d9789c74dc3161a26d608\\clang-5.0-py3-none-any.whl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: After October 2020 you may experience errors when installing or updating packages. This is because pip will change the way that it resolves dependency conflicts.\n",
      "\n",
      "We recommend you use --use-feature=2020-resolver to test your packages with the new resolver before it becomes the default.\n",
      "\n",
      "tensorflow 2.5.0 requires grpcio~=1.34.0, but you'll have grpcio 1.39.0 which is incompatible.\n",
      "tensorflow 2.5.0 requires tensorflow-estimator<2.6.0,>=2.5.0rc0, but you'll have tensorflow-estimator 2.6.0 which is incompatible.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting grpcio<2.0,>=1.37.0\n",
      "  Using cached grpcio-1.39.0-cp38-cp38-win_amd64.whl (3.2 MB)\n",
      "Requirement already satisfied, skipping upgrade: wrapt~=1.12.1 in c:\\users\\何佳馨\\appdata\\roaming\\python\\python38\\site-packages (from tensorflow-gpu) (1.12.1)\n",
      "Requirement already satisfied, skipping upgrade: termcolor~=1.1.0 in c:\\users\\何佳馨\\appdata\\roaming\\python\\python38\\site-packages (from tensorflow-gpu) (1.1.0)\n",
      "Requirement already satisfied, skipping upgrade: keras~=2.6 in c:\\amy\\anaconda3\\lib\\site-packages (from tensorflow-gpu) (2.6.0)\n",
      "Requirement already satisfied, skipping upgrade: numpy~=1.19.2 in c:\\amy\\anaconda3\\lib\\site-packages (from tensorflow-gpu) (1.19.2)\n",
      "Collecting tensorflow-estimator~=2.6\n",
      "  Using cached tensorflow_estimator-2.6.0-py2.py3-none-any.whl (462 kB)\n",
      "Requirement already satisfied, skipping upgrade: protobuf>=3.9.2 in c:\\users\\何佳馨\\appdata\\roaming\\python\\python38\\site-packages (from tensorflow-gpu) (3.17.3)\n",
      "Requirement already satisfied, skipping upgrade: google-pasta~=0.2 in c:\\users\\何佳馨\\appdata\\roaming\\python\\python38\\site-packages (from tensorflow-gpu) (0.2.0)\n",
      "Requirement already satisfied, skipping upgrade: h5py~=3.1.0 in c:\\users\\何佳馨\\appdata\\roaming\\python\\python38\\site-packages (from tensorflow-gpu) (3.1.0)\n",
      "Requirement already satisfied, skipping upgrade: six~=1.15.0 in c:\\amy\\anaconda3\\lib\\site-packages (from tensorflow-gpu) (1.15.0)\n",
      "Requirement already satisfied, skipping upgrade: flatbuffers~=1.12.0 in c:\\users\\何佳馨\\appdata\\roaming\\python\\python38\\site-packages (from tensorflow-gpu) (1.12)\n",
      "Requirement already satisfied, skipping upgrade: gast==0.4.0 in c:\\users\\何佳馨\\appdata\\roaming\\python\\python38\\site-packages (from tensorflow-gpu) (0.4.0)\n",
      "Requirement already satisfied, skipping upgrade: wheel~=0.35 in c:\\amy\\anaconda3\\lib\\site-packages (from tensorflow-gpu) (0.35.1)\n",
      "Requirement already satisfied, skipping upgrade: setuptools>=41.0.0 in c:\\amy\\anaconda3\\lib\\site-packages (from tensorboard~=2.6->tensorflow-gpu) (50.3.1.post20201107)\n",
      "Requirement already satisfied, skipping upgrade: tensorboard-plugin-wit>=1.6.0 in c:\\users\\何佳馨\\appdata\\roaming\\python\\python38\\site-packages (from tensorboard~=2.6->tensorflow-gpu) (1.8.0)\n",
      "Requirement already satisfied, skipping upgrade: requests<3,>=2.21.0 in c:\\amy\\anaconda3\\lib\\site-packages (from tensorboard~=2.6->tensorflow-gpu) (2.24.0)\n",
      "Requirement already satisfied, skipping upgrade: tensorboard-data-server<0.7.0,>=0.6.0 in c:\\users\\何佳馨\\appdata\\roaming\\python\\python38\\site-packages (from tensorboard~=2.6->tensorflow-gpu) (0.6.1)\n",
      "Requirement already satisfied, skipping upgrade: google-auth-oauthlib<0.5,>=0.4.1 in c:\\users\\何佳馨\\appdata\\roaming\\python\\python38\\site-packages (from tensorboard~=2.6->tensorflow-gpu) (0.4.4)\n",
      "Requirement already satisfied, skipping upgrade: markdown>=2.6.8 in c:\\users\\何佳馨\\appdata\\roaming\\python\\python38\\site-packages (from tensorboard~=2.6->tensorflow-gpu) (3.3.4)\n",
      "Requirement already satisfied, skipping upgrade: werkzeug>=0.11.15 in c:\\amy\\anaconda3\\lib\\site-packages (from tensorboard~=2.6->tensorflow-gpu) (1.0.1)\n",
      "Requirement already satisfied, skipping upgrade: google-auth<2,>=1.6.3 in c:\\users\\何佳馨\\appdata\\roaming\\python\\python38\\site-packages (from tensorboard~=2.6->tensorflow-gpu) (1.31.0)\n",
      "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in c:\\amy\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow-gpu) (3.0.4)\n",
      "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in c:\\amy\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow-gpu) (2.10)\n",
      "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in c:\\amy\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow-gpu) (2020.6.20)\n",
      "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\amy\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow-gpu) (1.25.11)\n",
      "Requirement already satisfied, skipping upgrade: requests-oauthlib>=0.7.0 in c:\\users\\何佳馨\\appdata\\roaming\\python\\python38\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow-gpu) (1.3.0)\n",
      "Requirement already satisfied, skipping upgrade: cachetools<5.0,>=2.0.0 in c:\\users\\何佳馨\\appdata\\roaming\\python\\python38\\site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.6->tensorflow-gpu) (4.2.2)\n",
      "Requirement already satisfied, skipping upgrade: pyasn1-modules>=0.2.1 in c:\\users\\何佳馨\\appdata\\roaming\\python\\python38\\site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.6->tensorflow-gpu) (0.2.8)\n",
      "Requirement already satisfied, skipping upgrade: rsa<5,>=3.1.4; python_version >= \"3.6\" in c:\\users\\何佳馨\\appdata\\roaming\\python\\python38\\site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.6->tensorflow-gpu) (4.7.2)\n",
      "Requirement already satisfied, skipping upgrade: oauthlib>=3.0.0 in c:\\users\\何佳馨\\appdata\\roaming\\python\\python38\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow-gpu) (3.1.1)\n",
      "Requirement already satisfied, skipping upgrade: pyasn1<0.5.0,>=0.4.6 in c:\\users\\何佳馨\\appdata\\roaming\\python\\python38\\site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard~=2.6->tensorflow-gpu) (0.4.8)\n",
      "Installing collected packages: grpcio, tensorboard, clang, tensorflow-estimator, tensorflow-gpu\n",
      "  Attempting uninstall: grpcio\n",
      "    Found existing installation: grpcio 1.34.1\n",
      "    Uninstalling grpcio-1.34.1:\n",
      "      Successfully uninstalled grpcio-1.34.1\n",
      "  Attempting uninstall: tensorboard\n",
      "    Found existing installation: tensorboard 2.5.0\n",
      "    Uninstalling tensorboard-2.5.0:\n",
      "      Successfully uninstalled tensorboard-2.5.0\n",
      "  Attempting uninstall: tensorflow-estimator\n",
      "    Found existing installation: tensorflow-estimator 2.5.0\n",
      "    Uninstalling tensorflow-estimator-2.5.0:\n",
      "      Successfully uninstalled tensorflow-estimator-2.5.0\n",
      "Successfully installed clang-5.0 grpcio-1.39.0 tensorboard-2.6.0 tensorflow-estimator-2.6.0 tensorflow-gpu-2.6.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement tensorflow-gpu==2.0.0-beta0 (from versions: 2.2.0rc1, 2.2.0rc2, 2.2.0rc3, 2.2.0rc4, 2.2.0, 2.2.1, 2.2.2, 2.2.3, 2.3.0rc0, 2.3.0rc1, 2.3.0rc2, 2.3.0, 2.3.1, 2.3.2, 2.3.3, 2.3.4, 2.4.0rc0, 2.4.0rc1, 2.4.0rc2, 2.4.0rc3, 2.4.0rc4, 2.4.0, 2.4.1, 2.4.2, 2.4.3, 2.5.0rc0, 2.5.0rc1, 2.5.0rc2, 2.5.0rc3, 2.5.0, 2.5.1, 2.6.0rc0, 2.6.0rc1, 2.6.0rc2, 2.6.0)\n",
      "ERROR: No matching distribution found for tensorflow-gpu==2.0.0-beta0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: keras in c:\\amy\\anaconda3\\lib\\site-packages (2.6.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade tensorflow-gpu\n",
    "!pip install tensorflow-gpu==2.0.0-beta0\n",
    "!pip install keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow_datasets\n",
      "  Downloading tensorflow_datasets-4.4.0-py3-none-any.whl (4.0 MB)\n",
      "Requirement already satisfied: absl-py in c:\\users\\何佳馨\\appdata\\roaming\\python\\python38\\site-packages (from tensorflow_datasets) (0.12.0)\n",
      "Collecting tensorflow-metadata\n",
      "  Downloading tensorflow_metadata-1.2.0-py3-none-any.whl (48 kB)\n",
      "Requirement already satisfied: attrs>=18.1.0 in c:\\amy\\anaconda3\\lib\\site-packages (from tensorflow_datasets) (20.3.0)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\amy\\anaconda3\\lib\\site-packages (from tensorflow_datasets) (2.24.0)\n",
      "Collecting promise\n",
      "  Downloading promise-2.3.tar.gz (19 kB)\n",
      "Requirement already satisfied: termcolor in c:\\users\\何佳馨\\appdata\\roaming\\python\\python38\\site-packages (from tensorflow_datasets) (1.1.0)\n",
      "Requirement already satisfied: protobuf>=3.12.2 in c:\\users\\何佳馨\\appdata\\roaming\\python\\python38\\site-packages (from tensorflow_datasets) (3.17.3)\n",
      "Requirement already satisfied: six in c:\\amy\\anaconda3\\lib\\site-packages (from tensorflow_datasets) (1.15.0)\n",
      "Requirement already satisfied: tqdm in c:\\amy\\anaconda3\\lib\\site-packages (from tensorflow_datasets) (4.50.2)\n",
      "Collecting dill\n",
      "  Downloading dill-0.3.4-py2.py3-none-any.whl (86 kB)\n",
      "Collecting importlib-resources; python_version < \"3.9\"\n",
      "  Downloading importlib_resources-5.2.2-py3-none-any.whl (27 kB)\n",
      "Requirement already satisfied: future in c:\\amy\\anaconda3\\lib\\site-packages (from tensorflow_datasets) (0.18.2)\n",
      "Requirement already satisfied: numpy in c:\\amy\\anaconda3\\lib\\site-packages (from tensorflow_datasets) (1.19.2)\n",
      "Collecting googleapis-common-protos<2,>=1.52.0\n",
      "  Downloading googleapis_common_protos-1.53.0-py2.py3-none-any.whl (198 kB)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\amy\\anaconda3\\lib\\site-packages (from requests>=2.19.0->tensorflow_datasets) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\amy\\anaconda3\\lib\\site-packages (from requests>=2.19.0->tensorflow_datasets) (1.25.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\amy\\anaconda3\\lib\\site-packages (from requests>=2.19.0->tensorflow_datasets) (2020.6.20)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\amy\\anaconda3\\lib\\site-packages (from requests>=2.19.0->tensorflow_datasets) (2.10)\n",
      "Requirement already satisfied: zipp>=3.1.0; python_version < \"3.10\" in c:\\amy\\anaconda3\\lib\\site-packages (from importlib-resources; python_version < \"3.9\"->tensorflow_datasets) (3.4.0)\n",
      "Building wheels for collected packages: promise\n",
      "  Building wheel for promise (setup.py): started\n",
      "  Building wheel for promise (setup.py): finished with status 'done'\n",
      "  Created wheel for promise: filename=promise-2.3-py3-none-any.whl size=21500 sha256=48f546c35b73569870f05ec2c6dc3370218dd77ff0111e8f19ebba8bacce2a06\n",
      "  Stored in directory: c:\\users\\何佳馨\\appdata\\local\\pip\\cache\\wheels\\54\\aa\\01\\724885182f93150035a2a91bce34a12877e8067a97baaf5dc8\n",
      "Successfully built promise\n",
      "Installing collected packages: googleapis-common-protos, tensorflow-metadata, promise, dill, importlib-resources, tensorflow-datasets\n",
      "Successfully installed dill-0.3.4 googleapis-common-protos-1.53.0 importlib-resources-5.2.2 promise-2.3 tensorflow-datasets-4.4.0 tensorflow-metadata-1.2.0\n",
      "2.5.0\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow_datasets\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(level=logging.ERROR)\n",
    "\n",
    "np.set_printoptions(suppress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"nmt\"\n",
    "en_vocab_file = os.path.join(output_dir, \"en_vocab\")\n",
    "zh_vocab_file = os.path.join(output_dir, \"zh_vocab\")\n",
    "checkpoint_path = os.path.join(output_dir, \"checkpoints\")\n",
    "log_dir = os.path.join(output_dir, 'logs')\n",
    "download_dir = \"tensorflow-datasets/downloads\"\n",
    "\n",
    "if not os.path.exists(output_dir):\n",
    "  os.makedirs(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_builder = tfds.builder(\"wmt19_translate/zh-en\")\n",
    "\n",
    "config = tfds.translate.wmt.WmtConfig(\n",
    "  version=tfds.core.Version('0.0.3', experiments={tfds.core.ReadInstruction: False}),\n",
    "  language_pair=(\"zh\", \"en\"),\n",
    "  subsets={\n",
    "    tfds.Split.TRAIN: [\"newscommentary_v14\"]\n",
    "  }\n",
    ")\n",
    "builder = tfds.builder(\"wmt_translate\", config=config)\n",
    "builder.download_and_prepare(download_dir=download_dir)\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['train[:20%]', 'train[20%:21%]', 'train[21%:]']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split = ['train[:20%]','train[20%:21%]','train[21%:]']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = builder.as_dataset(split=split, as_supervised=True)\n",
    "train_examples, val_examples, _ = examples\n",
    "\n",
    "sample_examples = []\n",
    "num_samples = 10\n",
    "\n",
    "for en_t, zh_t in train_examples.take(num_samples):\n",
    "  en = en_t.numpy().decode(\"utf-8\")\n",
    "  zh = zh_t.numpy().decode(\"utf-8\")\n",
    "\n",
    "    # 之後用來簡單評估模型的訓練情況\n",
    "  sample_examples.append((en, zh))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "沒有已建立的字典，從頭建立。\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "  subword_encoder_en = tfds.features.text.SubwordTextEncoder.load_from_file(en_vocab_file)\n",
    "  print(f\"載入已建立的字典： {en_vocab_file}\")\n",
    "except:\n",
    "  print(\"沒有已建立的字典，從頭建立。\")\n",
    "  subword_encoder_en = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(\n",
    "      (en.numpy() for en, _ in train_examples), \n",
    "      target_vocab_size=2**13) # 有需要可以調整字典大小\n",
    " \n",
    "  # 將字典檔案存下以方便下次 warmstart\n",
    "  subword_encoder_en.save_to_file(en_vocab_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "沒有已建立的字典，從頭建立。\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "  subword_encoder_zh = tfds.features.text.SubwordTextEncoder.load_from_file(zh_vocab_file)\n",
    "  print(f\"載入已建立的字典： {zh_vocab_file}\")\n",
    "except:\n",
    "  print(\"沒有已建立的字典，從頭建立。\")\n",
    "  subword_encoder_zh = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(\n",
    "      (zh.numpy() for _, zh in train_examples), \n",
    "      target_vocab_size=2**13, # 有需要可以調整字典大小\n",
    "      max_subword_length=1) # 每一個中文字就是字典裡的一個單位\n",
    "  \n",
    "  # 將字典檔案存下以方便下次 warmstart \n",
    "  subword_encoder_zh.save_to_file(zh_vocab_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(en_t, zh_t):\n",
    "  # 因為字典的索引從 0 開始，\n",
    "  # 我們可以使用 subword_encoder_en.vocab_size 這個值作為 BOS 的索引值\n",
    "  # 用 subword_encoder_en.vocab_size + 1 作為 EOS 的索引值\n",
    "  en_indices = [subword_encoder_en.vocab_size] + subword_encoder_en.encode(\n",
    "      en_t.numpy()) + [subword_encoder_en.vocab_size + 1]\n",
    "  # 同理，不過是使用中文字典的最後一個索引 + 1\n",
    "  zh_indices = [subword_encoder_zh.vocab_size] + subword_encoder_zh.encode(\n",
    "      zh_t.numpy()) + [subword_encoder_zh.vocab_size + 1]\n",
    "  \n",
    "  return en_indices, zh_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_t, zh_t = next(iter(train_examples))\n",
    "en_indices, zh_indices = encode(en_t, zh_t)\n",
    "\n",
    "def tf_encode(en_t, zh_t):\n",
    "  # 在 `tf_encode` 函式裡頭的 `en_t` 與 `zh_t` 都不是 Eager Tensors\n",
    "  # 要到 `tf.py_funtion` 裡頭才是\n",
    "  # 另外因為索引都是整數，所以使用 `tf.int64`\n",
    "  return tf.py_function(encode, [en_t, zh_t], [tf.int64, tf.int64])\n",
    "\n",
    "# `tmp_dataset` 為說明用資料集，說明完所有重要的 func，\n",
    "# 我們會從頭建立一個正式的 `train_dataset`\n",
    "tmp_dataset = train_examples.map(tf_encode)\n",
    "en_indices, zh_indices = next(iter(tmp_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 40\n",
    "\n",
    "def filter_max_length(en, zh, max_length=MAX_LENGTH):\n",
    "  # en, zh 分別代表英文與中文的索引序列\n",
    "  return tf.logical_and(tf.size(en) <= max_length,\n",
    "                        tf.size(zh) <= max_length)\n",
    "\n",
    "# tf.data.Dataset.filter(func) 只會回傳 func 為真的例子\n",
    "tmp_dataset = tmp_dataset.filter(filter_max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_examples = 0\n",
    "for en_indices, zh_indices in tmp_dataset:\n",
    "  cond1 = len(en_indices) <= MAX_LENGTH\n",
    "  cond2 = len(zh_indices) <= MAX_LENGTH\n",
    "  assert cond1 and cond2\n",
    "  num_examples += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "# 將 batch 裡的所有序列都 pad 到同樣長度\n",
    "tmp_dataset = tmp_dataset.padded_batch(BATCH_SIZE, padded_shapes=([-1], [-1]))\n",
    "en_batch, zh_batch = next(iter(tmp_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 40\n",
    "BATCH_SIZE = 128\n",
    "BUFFER_SIZE = 15000\n",
    "\n",
    "# 訓練集\n",
    "train_dataset = (train_examples  # 輸出：(英文句子, 中文句子)\n",
    "                 .map(tf_encode) # 輸出：(英文索引序列, 中文索引序列)\n",
    "                 .filter(filter_max_length) # 同上，且序列長度都不超過 40\n",
    "                 .cache() # 加快讀取數據\n",
    "                 .shuffle(BUFFER_SIZE) # 將例子洗牌確保隨機性\n",
    "                 .padded_batch(BATCH_SIZE, # 將 batch 裡的序列都 pad 到一樣長度\n",
    "                               padded_shapes=([-1], [-1]))\n",
    "                 .prefetch(tf.data.experimental.AUTOTUNE)) # 加速\n",
    "# 驗證集\n",
    "val_dataset = (val_examples\n",
    "               .map(tf_encode)\n",
    "               .filter(filter_max_length)\n",
    "               .padded_batch(BATCH_SIZE, \n",
    "                             padded_shapes=([-1], [-1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_batch, zh_batch = next(iter(train_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_examples = [\n",
    "    (\"It is important.\", \"这很重要。\"),\n",
    "    (\"The numbers speak for themselves.\", \"数字证明了一切。\"),\n",
    "]\n",
    "\n",
    "batch_size = 2\n",
    "demo_examples = tf.data.Dataset.from_tensor_slices((\n",
    "    [en for en, _ in demo_examples], [zh for _, zh in demo_examples]\n",
    "))\n",
    "\n",
    "# 將兩個句子透過之前定義的字典轉換成子詞的序列（sequence of subwords）\n",
    "# 並添加 padding token: <pad> 來確保 batch 裡的句子有一樣長度\n",
    "demo_dataset = demo_examples.map(tf_encode)\\\n",
    "  .padded_batch(batch_size, padded_shapes=([-1], [-1]))\n",
    "\n",
    "# 取出這個 demo dataset 裡唯一一個 batch\n",
    "inp, tar = next(iter(demo_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# + 2 是因為我們額外加了 <start> 以及 <end> tokens\n",
    "vocab_size_en = subword_encoder_en.vocab_size + 2\n",
    "vocab_size_zh = subword_encoder_zh.vocab_size + 2\n",
    "\n",
    "# 為了方便 demo, 將詞彙轉換到一個 4 維的詞嵌入空間\n",
    "d_model = 4\n",
    "embedding_layer_en = tf.keras.layers.Embedding(vocab_size_en, d_model)\n",
    "embedding_layer_zh = tf.keras.layers.Embedding(vocab_size_zh, d_model)\n",
    "\n",
    "emb_inp = embedding_layer_en(inp)\n",
    "emb_tar = embedding_layer_zh(tar)\n",
    "emb_inp, emb_tar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_padding_mask(seq):\n",
    "  # padding mask 的工作就是把索引序列中為 0 的位置設為 1\n",
    "  mask = tf.cast(tf.equal(seq, 0), tf.float32)\n",
    "  return mask[:, tf.newaxis, tf.newaxis, :] #　broadcasting\n",
    "\n",
    "inp_mask = create_padding_mask(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 設定一個 seed 確保我們每次都拿到一樣的隨機結果\n",
    "tf.random.set_seed(9527)\n",
    "\n",
    "# 自注意力機制：查詢 `q` 跟鍵值 `k` 都是 `emb_inp`\n",
    "q = emb_inp\n",
    "k = emb_inp\n",
    "# 簡單產生一個跟 `emb_inp` 同樣 shape 的 binary vector\n",
    "v = tf.cast(tf.math.greater(tf.random.uniform(shape=emb_inp.shape), 0.5), tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matmul_qk=0\n",
    "dk=0\n",
    "def scaled_dot_product_attention(q, k, v, mask):\n",
    "  global matmul_qk,dk\n",
    "  # 將 `q`、 `k` 做點積再 scale\n",
    "  matmul_qk = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k)\n",
    "  \n",
    "  dk = tf.cast(tf.shape(k)[-1], tf.float32)  # 取得 seq_k 的序列長度\n",
    "  scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)  # scale by sqrt(dk)\n",
    "\n",
    "  # 將遮罩「加」到被丟入 softmax 前的 logits\n",
    "  if mask is not None:\n",
    "    scaled_attention_logits += (mask * -1e9)\n",
    "\n",
    "  # 取 softmax 是為了得到總和為 1 的比例之後對 `v` 做加權平均\n",
    "  attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  # (..., seq_len_q, seq_len_k)\n",
    "  \n",
    "  # 以注意權重對 v 做加權平均（weighted average）\n",
    "  output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n",
    "  return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = None\n",
    "output, attention_weights = scaled_dot_product_attention(q, k, v, mask)\n",
    "\n",
    "# 將 `q`、 `k` 做點積再 scale\n",
    "scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
    "\n",
    "# 將遮罩「加」到被丟入 softmax 前的 logits\n",
    "if mask is not None:\n",
    "  scaled_attention_logits += (mask * -1e9)\n",
    "\n",
    "# 取 softmax 是為了得到總和為 1 的比例做加權平均\n",
    "attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if mask is not None:\n",
    "  scaled_attention_logits += (mask * -1e9) # 是 -1e9 不是 1e-9\n",
    "\n",
    "attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)\n",
    "\n",
    "def create_padding_mask(seq):\n",
    "  # padding mask 的工作就是把索引序列中為 0 的位置設為 1\n",
    "  mask = tf.cast(tf.equal(seq, 0), tf.float32)\n",
    "  return mask[:, tf.newaxis, tf.newaxis, :] #　broadcasting\n",
    "\n",
    "inp_mask = create_padding_mask(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = tf.squeeze(inp_mask, axis=1) # (batch_size, 1, seq_len_q)\n",
    "_, attention_weights = scaled_dot_product_attention(q, k, v, mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_look_ahead_mask(size):\n",
    "  mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
    "  return mask  # (seq_len, seq_len)\n",
    "\n",
    "seq_len = emb_tar.shape[1] # 注意這次我們用中文的詞嵌入張量 `emb_tar`\n",
    "look_ahead_mask = create_look_ahead_mask(seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 讓我們用目標語言（中文）的 batch\n",
    "# 來模擬 Decoder 處理的情況\n",
    "temp_q = temp_k = emb_tar\n",
    "temp_v = tf.cast(tf.math.greater(\n",
    "    tf.random.uniform(shape=emb_tar.shape), 0.5), tf.float32)\n",
    "\n",
    "# 將 look_ahead_mask 放入注意函式\n",
    "_, attention_weights = scaled_dot_product_attention(\n",
    "    temp_q, temp_k, temp_v, look_ahead_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_heads(x, d_model, num_heads):\n",
    "  # x.shape: (batch_size, seq_len, d_model)\n",
    "  batch_size = tf.shape(x)[0]\n",
    "  \n",
    "  # 我們要確保維度 `d_model` 可以被平分成 `num_heads` 個 `depth` 維度\n",
    "  assert d_model % num_heads == 0\n",
    "  depth = d_model // num_heads  # 這是分成多頭以後每個向量的維度 \n",
    "  \n",
    "  # 將最後一個 d_model 維度分成 num_heads 個 depth 維度。\n",
    "  # 最後一個維度變成兩個維度，張量 x 從 3 維到 4 維\n",
    "  # (batch_size, seq_len, num_heads, depth)\n",
    "  reshaped_x = tf.reshape(x, shape=(batch_size, -1, num_heads, depth))\n",
    "  \n",
    "  # 將 head 的維度拉前使得最後兩個維度為子詞以及其對應的 depth 向量\n",
    "  # (batch_size, num_heads, seq_len, depth)\n",
    "  output = tf.transpose(reshaped_x, perm=[0, 2, 1, 3])\n",
    "  \n",
    "  return output\n",
    "\n",
    "# 我們的 `emb_inp` 裡頭的子詞本來就是 4 維的詞嵌入向量\n",
    "d_model = 4\n",
    "# 將 4 維詞嵌入向量分為 2 個 head 的 2 維矩陣\n",
    "num_heads = 2\n",
    "x = emb_inp\n",
    "\n",
    "output = split_heads(x, d_model, num_heads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 實作一個執行多頭注意力機制的 keras layer\n",
    "# 在初始的時候指定輸出維度 `d_model` & `num_heads，\n",
    "# 在呼叫的時候輸入 `v`, `k`, `q` 以及 `mask`\n",
    "# 輸出跟 scaled_dot_product_attention 函式一樣有兩個：\n",
    "# output.shape            == (batch_size, seq_len_q, d_model)\n",
    "# attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n",
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "  # 在初始的時候建立一些必要參數\n",
    "  def __init__(self, d_model, num_heads):\n",
    "    super(MultiHeadAttention, self).__init__()\n",
    "    self.num_heads = num_heads # 指定要將 `d_model` 拆成幾個 heads\n",
    "    self.d_model = d_model # 在 split_heads 之前的基底維度\n",
    "    \n",
    "    assert d_model % self.num_heads == 0  # 前面看過，要確保可以平分\n",
    "    \n",
    "    self.depth = d_model // self.num_heads  # 每個 head 裡子詞的新的 repr. 維度\n",
    "    \n",
    "    self.wq = tf.keras.layers.Dense(d_model)  # 分別給 q, k, v 的 3 個線性轉換 \n",
    "    self.wk = tf.keras.layers.Dense(d_model)  # 注意我們並沒有指定 activation func\n",
    "    self.wv = tf.keras.layers.Dense(d_model)\n",
    "    \n",
    "    self.dense = tf.keras.layers.Dense(d_model)  # 多 heads 串接後通過的線性轉換\n",
    "  \n",
    "  # 這跟我們前面看過的函式有 87% 相似\n",
    "  def split_heads(self, x, batch_size):\n",
    "    \"\"\"Split the last dimension into (num_heads, depth).\n",
    "    Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
    "    \"\"\"\n",
    "    x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
    "    return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "  \n",
    "  # multi-head attention 的實際執行流程，注意參數順序（這邊跟論文以及 TensorFlow 官方教學一致）\n",
    "  def call(self, v, k, q, mask):\n",
    "    batch_size = tf.shape(q)[0]\n",
    "    \n",
    "    # 將輸入的 q, k, v 都各自做一次線性轉換到 `d_model` 維空間\n",
    "    q = self.wq(q)  # (batch_size, seq_len, d_model)\n",
    "    k = self.wk(k)  # (batch_size, seq_len, d_model)\n",
    "    v = self.wv(v)  # (batch_size, seq_len, d_model)\n",
    "    \n",
    "    # 前面看過的，將最後一個 `d_model` 維度分成 `num_heads` 個 `depth` 維度\n",
    "    q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n",
    "    k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n",
    "    v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n",
    "    \n",
    "    # 利用 broadcasting 讓每個句子的每個 head 的 qi, ki, vi 都各自進行注意力機制\n",
    "    # 輸出會多一個 head 維度\n",
    "    scaled_attention, attention_weights = scaled_dot_product_attention(\n",
    "        q, k, v, mask)\n",
    "    # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\n",
    "    # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n",
    "    \n",
    "    # 跟我們在 `split_heads` 函式做的事情剛好相反，先做 transpose 再做 reshape\n",
    "    # 將 `num_heads` 個 `depth` 維度串接回原來的 `d_model` 維度\n",
    "    scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])\n",
    "    # (batch_size, seq_len_q, num_heads, depth)\n",
    "    concat_attention = tf.reshape(scaled_attention, \n",
    "                                  (batch_size, -1, self.d_model)) \n",
    "    # (batch_size, seq_len_q, d_model)\n",
    "\n",
    "    # 通過最後一個線性轉換\n",
    "    output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\n",
    "        \n",
    "    return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# emb_inp.shape == (batch_size, seq_len, d_model)\n",
    "#               == (2, 8, 4)\n",
    "assert d_model == emb_inp.shape[-1]  == 4\n",
    "num_heads = 2\n",
    "\n",
    "# 初始化一個 multi-head attention layer\n",
    "mha = MultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "# 簡單將 v, k, q 都設置為 `emb_inp`\n",
    "# 順便看看 padding mask 的作用。\n",
    "# 別忘記，第一個英文序列的最後兩個 tokens 是 <pad>\n",
    "v = k = q = emb_inp\n",
    "padding_mask = create_padding_mask(inp)\n",
    "\n",
    "output, attention_weights = mha(v, k, q, mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 建立 Transformer 裡 Encoder / Decoder layer 都有使用到的 Feed Forward 元件\n",
    "def point_wise_feed_forward_network(d_model, dff):\n",
    "  \n",
    "  # 此 FFN 對輸入做兩個線性轉換，中間加了一個 ReLU activation func\n",
    "  return tf.keras.Sequential([\n",
    "      tf.keras.layers.Dense(dff, activation='relu'),  # (batch_size, seq_len, dff)\n",
    "      tf.keras.layers.Dense(d_model)  # (batch_size, seq_len, d_model)\n",
    "  ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "seq_len = 10\n",
    "d_model = 512\n",
    "dff = 2048\n",
    "\n",
    "x = tf.random.uniform((batch_size, seq_len, d_model))\n",
    "ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "out = ffn(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_model = 4 # FFN 的輸入輸出張量的最後一維皆為 `d_model`\n",
    "dff = 6\n",
    "\n",
    "# 建立一個小 FFN\n",
    "small_ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "# 懂子詞梗的站出來\n",
    "dummy_sentence = tf.constant([[5, 5, 6, 6], \n",
    "                              [5, 5, 6, 6], \n",
    "                              [9, 5, 2, 7], \n",
    "                              [9, 5, 2, 7],\n",
    "                              [9, 5, 2, 7]], dtype=tf.float32)\n",
    "small_ffn(dummy_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder 裡頭會有 N 個 EncoderLayers，而每個 EncoderLayer 裡又有兩個 sub-layers: MHA & FFN\n",
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "  # Transformer 論文內預設 dropout rate 為 0.1\n",
    "  def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "    super(EncoderLayer, self).__init__()\n",
    "\n",
    "    self.mha = MultiHeadAttention(d_model, num_heads)\n",
    "    self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "\n",
    "    # layer norm 很常在 RNN-based 的模型被使用。一個 sub-layer 一個 layer norm\n",
    "    self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "    self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "    \n",
    "    # 一樣，一個 sub-layer 一個 dropout layer\n",
    "    self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "    self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "    \n",
    "  # 需要丟入 `training` 參數是因為 dropout 在訓練以及測試的行為有所不同\n",
    "  def call(self, x, training, mask):\n",
    "    # 除了 `attn`，其他張量的 shape 皆為 (batch_size, input_seq_len, d_model)\n",
    "    # attn.shape == (batch_size, num_heads, input_seq_len, input_seq_len)\n",
    "    \n",
    "    # sub-layer 1: MHA\n",
    "    # Encoder 利用注意機制關注自己當前的序列，因此 v, k, q 全部都是自己\n",
    "    # 另外別忘了我們還需要 padding mask 來遮住輸入序列中的 <pad> token\n",
    "    attn_output, attn = self.mha(x, x, x, mask)  \n",
    "    attn_output = self.dropout1(attn_output, training=training) \n",
    "    out1 = self.layernorm1(x + attn_output)  \n",
    "    \n",
    "    # sub-layer 2: FFN\n",
    "    ffn_output = self.ffn(out1) \n",
    "    ffn_output = self.dropout2(ffn_output, training=training)  # 記得 training\n",
    "    out2 = self.layernorm2(out1 + ffn_output)\n",
    "    \n",
    "    return out2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 之後可以調的超參數。這邊為了 demo 設小一點\n",
    "d_model = 4\n",
    "num_heads = 2\n",
    "dff = 8\n",
    "\n",
    "# 新建一個使用上述參數的 Encoder Layer\n",
    "enc_layer = EncoderLayer(d_model, num_heads, dff)\n",
    "padding_mask = create_padding_mask(inp)  # 建立一個當前輸入 batch 使用的 padding mask\n",
    "enc_out = enc_layer(emb_inp, training=False, mask=padding_mask)  # (batch_size, seq_len, d_model)\n",
    "\n",
    "assert emb_inp.shape == enc_out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decoder 裡頭會有 N 個 DecoderLayer，\n",
    "# 而 DecoderLayer 又有三個 sub-layers: 自注意的 MHA, 關注 Encoder 輸出的 MHA & FFN\n",
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "  def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "    super(DecoderLayer, self).__init__()\n",
    "\n",
    "    # 3 個 sub-layers 的主角們\n",
    "    self.mha1 = MultiHeadAttention(d_model, num_heads)\n",
    "    self.mha2 = MultiHeadAttention(d_model, num_heads)\n",
    "    self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    " \n",
    "    # 定義每個 sub-layer 用的 LayerNorm\n",
    "    self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "    self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "    self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "    \n",
    "    # 定義每個 sub-layer 用的 Dropout\n",
    "    self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "    self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "    self.dropout3 = tf.keras.layers.Dropout(rate)\n",
    "    \n",
    "    \n",
    "  def call(self, x, enc_output, training, \n",
    "           combined_mask, inp_padding_mask):\n",
    "    # 所有 sub-layers 的主要輸出皆為 (batch_size, target_seq_len, d_model)\n",
    "    # enc_output 為 Encoder 輸出序列，shape 為 (batch_size, input_seq_len, d_model)\n",
    "    # attn_weights_block_1 則為 (batch_size, num_heads, target_seq_len, target_seq_len)\n",
    "    # attn_weights_block_2 則為 (batch_size, num_heads, target_seq_len, input_seq_len)\n",
    "\n",
    "    # sub-layer 1: Decoder layer 自己對輸出序列做注意力。\n",
    "    # 我們同時需要 look ahead mask 以及輸出序列的 padding mask \n",
    "    # 來避免前面已生成的子詞關注到未來的子詞以及 <pad>\n",
    "    attn1, attn_weights_block1 = self.mha1(x, x, x, combined_mask)\n",
    "    attn1 = self.dropout1(attn1, training=training)\n",
    "    out1 = self.layernorm1(attn1 + x)\n",
    "    \n",
    "    # sub-layer 2: Decoder layer 關注 Encoder 的最後輸出\n",
    "    # 記得我們一樣需要對 Encoder 的輸出套用 padding mask 避免關注到 <pad>\n",
    "    attn2, attn_weights_block2 = self.mha2(\n",
    "        enc_output, enc_output, out1, inp_padding_mask)  # (batch_size, target_seq_len, d_model)\n",
    "    attn2 = self.dropout2(attn2, training=training)\n",
    "    out2 = self.layernorm2(attn2 + out1)  # (batch_size, target_seq_len, d_model)\n",
    "    \n",
    "    # sub-layer 3: FFN 部分跟 Encoder layer 完全一樣\n",
    "    ffn_output = self.ffn(out2)  # (batch_size, target_seq_len, d_model)\n",
    "\n",
    "    ffn_output = self.dropout3(ffn_output, training=training)\n",
    "    out3 = self.layernorm3(ffn_output + out2)  # (batch_size, target_seq_len, d_model)\n",
    "    \n",
    "    # 除了主要輸出 `out3` 以外，輸出 multi-head 注意權重方便之後理解模型內部狀況\n",
    "    return out3, attn_weights_block1, attn_weights_block2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tar_padding_mask = create_padding_mask(tar)\n",
    "look_ahead_mask = create_look_ahead_mask(tar.shape[-1])\n",
    "combined_mask = tf.maximum(tar_padding_mask, look_ahead_mask)\n",
    "\n",
    "# 超參數\n",
    "d_model = 4\n",
    "num_heads = 2\n",
    "dff = 8\n",
    "dec_layer = DecoderLayer(d_model, num_heads, dff)\n",
    "\n",
    "# 來源、目標語言的序列都需要 padding mask\n",
    "inp_padding_mask = create_padding_mask(inp)\n",
    "tar_padding_mask = create_padding_mask(tar)\n",
    "\n",
    "# masked MHA 用的遮罩，把 padding 跟未來子詞都蓋住\n",
    "look_ahead_mask = create_look_ahead_mask(tar.shape[-1])\n",
    "combined_mask = tf.maximum(tar_padding_mask, look_ahead_mask)\n",
    "\n",
    "# 實際初始一個 decoder layer 並做 3 個 sub-layers 的計算\n",
    "dec_out, dec_self_attn_weights, dec_enc_attn_weights = dec_layer(\n",
    "    emb_tar, enc_out, False, combined_mask, inp_padding_mask)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 以下直接參考 TensorFlow 官方 tutorial \n",
    "def get_angles(pos, i, d_model):\n",
    "  angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(d_model))\n",
    "  return pos * angle_rates\n",
    "\n",
    "def positional_encoding(position, d_model):\n",
    "  angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n",
    "                          np.arange(d_model)[np.newaxis, :],\n",
    "                          d_model)\n",
    "  \n",
    "  # apply sin to even indices in the array; 2i\n",
    "  sines = np.sin(angle_rads[:, 0::2])\n",
    "  \n",
    "  # apply cos to odd indices in the array; 2i+1\n",
    "  cosines = np.cos(angle_rads[:, 1::2])\n",
    "  \n",
    "  pos_encoding = np.concatenate([sines, cosines], axis=-1)\n",
    "  \n",
    "  pos_encoding = pos_encoding[np.newaxis, ...]\n",
    "    \n",
    "  return tf.cast(pos_encoding, dtype=tf.float32)\n",
    "\n",
    "\n",
    "seq_len = 50\n",
    "d_model = 512\n",
    "\n",
    "pos_encoding = positional_encoding(seq_len, d_model)\n",
    "pos_encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.layers.Layer):\n",
    "  # Encoder 的初始參數除了本來就要給 EncoderLayer 的參數還多了：\n",
    "  # - num_layers: 決定要有幾個 EncoderLayers, 前面影片中的 `N`\n",
    "  # - input_vocab_size: 用來把索引轉成詞嵌入向量\n",
    "  def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, \n",
    "               rate=0.1):\n",
    "    super(Encoder, self).__init__()\n",
    "\n",
    "    self.d_model = d_model\n",
    "    \n",
    "    self.embedding = tf.keras.layers.Embedding(input_vocab_size, d_model)\n",
    "    self.pos_encoding = positional_encoding(input_vocab_size, self.d_model)\n",
    "    \n",
    "    # 建立 `num_layers` 個 EncoderLayers\n",
    "    self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate) \n",
    "                       for _ in range(num_layers)]\n",
    "\n",
    "    self.dropout = tf.keras.layers.Dropout(rate)\n",
    "        \n",
    "  def call(self, x, training, mask):\n",
    "    # 輸入的 x.shape == (batch_size, input_seq_len)\n",
    "    # 以下各 layer 的輸出皆為 (batch_size, input_seq_len, d_model)\n",
    "    input_seq_len = tf.shape(x)[1]\n",
    "    \n",
    "    # 將 2 維的索引序列轉成 3 維的詞嵌入張量，並依照論文乘上 sqrt(d_model)\n",
    "    # 再加上對應長度的位置編碼\n",
    "    x = self.embedding(x)\n",
    "    x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "    x += self.pos_encoding[:, :input_seq_len, :]\n",
    "\n",
    "    # 對 embedding 跟位置編碼的總合做 regularization\n",
    "    # 這在 Decoder 也會做\n",
    "    x = self.dropout(x, training=training)\n",
    "    \n",
    "    # 通過 N 個 EncoderLayer 做編碼\n",
    "    for i, enc_layer in enumerate(self.enc_layers):\n",
    "      x = enc_layer(x, training, mask)\n",
    "      # 以下只是用來 demo EncoderLayer outputs\n",
    "      #print('-' * 20)\n",
    "      #print(f\"EncoderLayer {i + 1}'s output:\", x)\n",
    "      \n",
    "    \n",
    "    return x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 超參數\n",
    "num_layers = 2 # 2 層的 Encoder\n",
    "d_model = 4\n",
    "num_heads = 2\n",
    "dff = 8\n",
    "input_vocab_size = subword_encoder_en.vocab_size + 2 # 記得加上 <start>, <end>\n",
    "\n",
    "# 初始化一個 Encoder\n",
    "encoder = Encoder(num_layers, d_model, num_heads, dff, input_vocab_size)\n",
    "\n",
    "# 將 2 維的索引序列丟入 Encoder 做編碼\n",
    "enc_out = encoder(inp, training=False, mask=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.layers.Layer):\n",
    "  # 初始參數跟 Encoder 只差在用 `target_vocab_size` 而非 `inp_vocab_size`\n",
    "  def __init__(self, num_layers, d_model, num_heads, dff, target_vocab_size, \n",
    "               rate=0.1):\n",
    "    super(Decoder, self).__init__()\n",
    "\n",
    "    self.d_model = d_model\n",
    "    \n",
    "    # 為中文（目標語言）建立詞嵌入層\n",
    "    self.embedding = tf.keras.layers.Embedding(target_vocab_size, d_model)\n",
    "    self.pos_encoding = positional_encoding(target_vocab_size, self.d_model)\n",
    "    \n",
    "    self.dec_layers = [DecoderLayer(d_model, num_heads, dff, rate) \n",
    "                       for _ in range(num_layers)]\n",
    "    self.dropout = tf.keras.layers.Dropout(rate)\n",
    "  \n",
    "  # 呼叫時的參數跟 DecoderLayer 一模一樣\n",
    "  def call(self, x, enc_output, training, \n",
    "           combined_mask, inp_padding_mask):\n",
    "    \n",
    "    tar_seq_len = tf.shape(x)[1]\n",
    "    attention_weights = {}  # 用來存放每個 Decoder layer 的注意權重\n",
    "    \n",
    "    # 這邊跟 Encoder 做的事情完全一樣\n",
    "    x = self.embedding(x)  # (batch_size, tar_seq_len, d_model)\n",
    "    x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "    x += self.pos_encoding[:, :tar_seq_len, :]\n",
    "    x = self.dropout(x, training=training)\n",
    "\n",
    "    \n",
    "    for i, dec_layer in enumerate(self.dec_layers):\n",
    "      x, block1, block2 = dec_layer(x, enc_output, training,\n",
    "                                    combined_mask, inp_padding_mask)\n",
    "      \n",
    "      # 將從每個 Decoder layer 取得的注意權重全部存下來回傳，方便我們觀察\n",
    "      attention_weights['decoder_layer{}_block1'.format(i + 1)] = block1\n",
    "      attention_weights['decoder_layer{}_block2'.format(i + 1)] = block2\n",
    "    \n",
    "    # x.shape == (batch_size, tar_seq_len, d_model)\n",
    "    return x, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 超參數\n",
    "num_layers = 2 # 2 層的 Decoder\n",
    "d_model = 4\n",
    "num_heads = 2\n",
    "dff = 8\n",
    "target_vocab_size = subword_encoder_zh.vocab_size + 2 # 記得加上 <start>, <end>\n",
    "\n",
    "# 遮罩\n",
    "inp_padding_mask = create_padding_mask(inp)\n",
    "tar_padding_mask = create_padding_mask(tar)\n",
    "look_ahead_mask = create_look_ahead_mask(tar.shape[1])\n",
    "combined_mask = tf.math.maximum(tar_padding_mask, look_ahead_mask)\n",
    "\n",
    "# 初始化一個 Decoder\n",
    "decoder = Decoder(num_layers, d_model, num_heads, dff, target_vocab_size)\n",
    "\n",
    "# 將 2 維的索引序列以及遮罩丟入 Decoder\n",
    "dec_out, attn = decoder(tar, enc_out, training=False, \n",
    "                        combined_mask=combined_mask,\n",
    "                        inp_padding_mask=inp_padding_mask)\n",
    "for block_name, attn_weights in attn.items():\n",
    "  print(f\"{block_name}.shape: {attn_weights.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformer 之上已經沒有其他 layers 了，我們使用 tf.keras.Model 建立一個模型\n",
    "class Transformer(tf.keras.Model):\n",
    "  # 初始參數包含 Encoder & Decoder 都需要超參數以及中英字典數目\n",
    "  def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, \n",
    "               target_vocab_size, rate=0.1):\n",
    "    super(Transformer, self).__init__()\n",
    "\n",
    "    self.encoder = Encoder(num_layers, d_model, num_heads, dff, \n",
    "                           input_vocab_size, rate)\n",
    "\n",
    "    self.decoder = Decoder(num_layers, d_model, num_heads, dff, \n",
    "                           target_vocab_size, rate)\n",
    "    # 這個 FFN 輸出跟中文字典一樣大的 logits 數，等通過 softmax 就代表每個中文字的出現機率\n",
    "    self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n",
    "  \n",
    "  # enc_padding_mask 跟 dec_padding_mask 都是英文序列的 padding mask，\n",
    "  # 只是一個給 Encoder layer 的 MHA 用，一個是給 Decoder layer 的 MHA 2 使用\n",
    "  def call(self, inp, tar, training, enc_padding_mask, \n",
    "           combined_mask, dec_padding_mask):\n",
    "\n",
    "    enc_output = self.encoder(inp, training, enc_padding_mask)  # (batch_size, inp_seq_len, d_model)\n",
    "    \n",
    "    # dec_output.shape == (batch_size, tar_seq_len, d_model)\n",
    "    dec_output, attention_weights = self.decoder(\n",
    "        tar, enc_output, training, combined_mask, dec_padding_mask)\n",
    "    \n",
    "    # 將 Decoder 輸出通過最後一個 linear layer\n",
    "    final_output = self.final_layer(dec_output)  # (batch_size, tar_seq_len, target_vocab_size)\n",
    "    \n",
    "    return final_output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 超參數\n",
    "num_layers = 1\n",
    "d_model = 4\n",
    "num_heads = 2\n",
    "dff = 8\n",
    "\n",
    "# + 2 是為了 <start> & <end> token\n",
    "input_vocab_size = subword_encoder_en.vocab_size + 2\n",
    "output_vocab_size = subword_encoder_zh.vocab_size + 2\n",
    "\n",
    "# 重點中的重點。訓練時用前一個字來預測下一個中文字\n",
    "tar_inp = tar[:, :-1]\n",
    "tar_real = tar[:, 1:]\n",
    "\n",
    "# 來源 / 目標語言用的遮罩。注意 `comined_mask` 已經將目標語言的兩種遮罩合而為一\n",
    "inp_padding_mask = create_padding_mask(inp)\n",
    "tar_padding_mask = create_padding_mask(tar_inp)\n",
    "look_ahead_mask = create_look_ahead_mask(tar_inp.shape[1])\n",
    "combined_mask = tf.math.maximum(tar_padding_mask, look_ahead_mask)\n",
    "\n",
    "# 初始化我們的第一個 transformer\n",
    "transformer = Transformer(num_layers, d_model, num_heads, dff, \n",
    "                          input_vocab_size, output_vocab_size)\n",
    "\n",
    "# 將英文、中文序列丟入取得 Transformer 預測下個中文字的結果\n",
    "predictions, attn_weights = transformer(inp, tar_inp, False, inp_padding_mask, \n",
    "                                        combined_mask, inp_padding_mask)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "# 假設我們要解的是一個 binary classifcation， 0 跟 1 個代表一個 label\n",
    "real = tf.constant([1, 1, 0], shape=(1, 3), dtype=tf.float32)\n",
    "pred = tf.constant([[0, 1], [0, 1], [0, 1]], dtype=tf.float32)\n",
    "loss_object(real, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(real, pred):\n",
    "  # 這次的 mask 將序列中不等於 0 的位置視為 1，其餘為 0 \n",
    "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "  # 照樣計算所有位置的 cross entropy 但不加總\n",
    "  loss_ = loss_object(real, pred)\n",
    "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "  loss_ *= mask  # 只計算非 <pad> 位置的損失 \n",
    "  \n",
    "  return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(\n",
    "    name='train_accuracy')\n",
    "\n",
    "num_layers = 4 \n",
    "d_model = 128\n",
    "dff = 512\n",
    "num_heads = 8\n",
    "\n",
    "input_vocab_size = subword_encoder_en.vocab_size + 2\n",
    "target_vocab_size = subword_encoder_zh.vocab_size + 2\n",
    "dropout_rate = 0.1  # 預設值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "  # 論文預設 `warmup_steps` = 4000\n",
    "  def __init__(self, d_model, warmup_steps=4000):\n",
    "    super(CustomSchedule, self).__init__()\n",
    "    \n",
    "    self.d_model = d_model\n",
    "    self.d_model = tf.cast(self.d_model, tf.float32)\n",
    "\n",
    "    self.warmup_steps = warmup_steps\n",
    "    \n",
    "  def __call__(self, step):\n",
    "    arg1 = tf.math.rsqrt(step)\n",
    "    arg2 = step * (self.warmup_steps ** -1.5)\n",
    "    \n",
    "    return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n",
    "  \n",
    "# 將客製化 learning rate schdeule 丟入 Adam opt.\n",
    "# Adam opt. 的參數都跟論文相同\n",
    "learning_rate = CustomSchedule(d_model)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, \n",
    "                                     epsilon=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_models = [128, 256, 512]\n",
    "warmup_steps = [1000 * i for i in range(1, 4)]\n",
    "\n",
    "schedules = []\n",
    "labels = []\n",
    "colors = [\"blue\", \"red\", \"black\"]\n",
    "for d in d_models:\n",
    "  schedules += [CustomSchedule(d, s) for s in warmup_steps]\n",
    "  labels += [f\"d_model: {d}, warm: {s}\" for s in warmup_steps]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = Transformer(num_layers, d_model, num_heads, dff,\n",
    "                          input_vocab_size, target_vocab_size, dropout_rate)\n",
    "\n",
    "# 方便比較不同實驗/ 不同超參數設定的結果\n",
    "run_id = f\"{num_layers}layers_{d_model}d_{num_heads}heads_{dff}dff_{train_perc}train_perc\"\n",
    "checkpoint_path = os.path.join(checkpoint_path, run_id)\n",
    "log_dir = os.path.join(log_dir, run_id)\n",
    "\n",
    "# tf.train.Checkpoint 可以幫我們把想要存下來的東西整合起來，方便儲存與讀取\n",
    "# 一般來說你會想存下模型以及 optimizer 的狀態\n",
    "ckpt = tf.train.Checkpoint(transformer=transformer,\n",
    "                           optimizer=optimizer)\n",
    "\n",
    "# ckpt_manager 會去 checkpoint_path 看有沒有符合 ckpt 裡頭定義的東西\n",
    "# 存檔的時候只保留最近 5 次 checkpoints，其他自動刪除\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n",
    "\n",
    "# 如果在 checkpoint 路徑上有發現檔案就讀進來\n",
    "if ckpt_manager.latest_checkpoint:\n",
    "  ckpt.restore(ckpt_manager.latest_checkpoint)\n",
    "  \n",
    "  # 用來確認之前訓練多少 epochs 了\n",
    "  last_epoch = int(ckpt_manager.latest_checkpoint.split(\"-\")[-1])\n",
    "  print(f'已讀取最新的 checkpoint，模型已訓練 {last_epoch} epochs。')\n",
    "else:\n",
    "  last_epoch = 0\n",
    "  print(\"沒找到 checkpoint，從頭訓練。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 為 Transformer 的 Encoder / Decoder 準備遮罩\n",
    "def create_masks(inp, tar):\n",
    "  # 英文句子的 padding mask，要交給 Encoder layer 自注意力機制用的\n",
    "  enc_padding_mask = create_padding_mask(inp)\n",
    "  \n",
    "  # 同樣也是英文句子的 padding mask，但是是要交給 Decoder layer 的 MHA 2 \n",
    "  # 關注 Encoder 輸出序列用的\n",
    "  dec_padding_mask = create_padding_mask(inp)\n",
    "  \n",
    "  # Decoder layer 的 MHA1 在做自注意力機制用的\n",
    "  # `combined_mask` 是中文句子的 padding mask 跟 look ahead mask 的疊加\n",
    "  look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])\n",
    "  dec_target_padding_mask = create_padding_mask(tar)\n",
    "  combined_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n",
    "  \n",
    "  return enc_padding_mask, combined_mask, dec_padding_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function  # 讓 TensorFlow 幫我們將 eager code 優化並加快運算\n",
    "def train_step(inp, tar):\n",
    "  # 前面說過的，用去尾的原始序列去預測下一個字的序列\n",
    "  tar_inp = tar[:, :-1]\n",
    "  tar_real = tar[:, 1:]\n",
    "  \n",
    "  # 建立 3 個遮罩\n",
    "  enc_padding_mask, combined_mask, dec_padding_mask = create_masks(inp, tar_inp)\n",
    "  \n",
    "  # 紀錄 Transformer 的所有運算過程以方便之後做梯度下降\n",
    "  with tf.GradientTape() as tape:\n",
    "    # 注意是丟入 `tar_inp` 而非 `tar`。記得將 `training` 參數設定為 True\n",
    "    predictions, _ = transformer(inp, tar_inp, \n",
    "                                 True, \n",
    "                                 enc_padding_mask, \n",
    "                                 combined_mask, \n",
    "                                 dec_padding_mask)\n",
    "    # 跟影片中顯示的相同，計算左移一個字的序列跟模型預測分佈之間的差異，當作 loss\n",
    "    loss = loss_function(tar_real, predictions)\n",
    "\n",
    "  # 取出梯度並呼叫前面定義的 Adam optimizer 幫我們更新 Transformer 裡頭可訓練的參數\n",
    "  gradients = tape.gradient(loss, transformer.trainable_variables)    \n",
    "  optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
    "  \n",
    "  # 將 loss 以及訓練 acc 記錄到 TensorBoard 上，非必要\n",
    "  train_loss(loss)\n",
    "  train_accuracy(tar_real, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function  # 讓 TensorFlow 幫我們將 eager code 優化並加快運算\n",
    "def train_step(inp, tar):\n",
    "  # 前面說過的，用去尾的原始序列去預測下一個字的序列\n",
    "  tar_inp = tar[:, :-1]\n",
    "  tar_real = tar[:, 1:]\n",
    "  \n",
    "  # 建立 3 個遮罩\n",
    "  enc_padding_mask, combined_mask, dec_padding_mask = create_masks(inp, tar_inp)\n",
    "  \n",
    "  # 紀錄 Transformer 的所有運算過程以方便之後做梯度下降\n",
    "  with tf.GradientTape() as tape:\n",
    "    # 注意是丟入 `tar_inp` 而非 `tar`。記得將 `training` 參數設定為 True\n",
    "    predictions, _ = transformer(inp, tar_inp, \n",
    "                                 True, \n",
    "                                 enc_padding_mask, \n",
    "                                 combined_mask, \n",
    "                                 dec_padding_mask)\n",
    "    # 跟影片中顯示的相同，計算左移一個字的序列跟模型預測分佈之間的差異，當作 loss\n",
    "    loss = loss_function(tar_real, predictions)\n",
    "\n",
    "  # 取出梯度並呼叫前面定義的 Adam optimizer 幫我們更新 Transformer 裡頭可訓練的參數\n",
    "  gradients = tape.gradient(loss, transformer.trainable_variables)    \n",
    "  optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
    "  \n",
    "  # 將 loss 以及訓練 acc 記錄到 TensorBoard 上，非必要\n",
    "  train_loss(loss)\n",
    "  train_accuracy(tar_real, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 給定一個英文句子，輸出預測的中文索引數字序列以及注意權重 dict\n",
    "def evaluate(inp_sentence):\n",
    "  \n",
    "  # 準備英文句子前後會加上的 <start>, <end>\n",
    "  start_token = [subword_encoder_en.vocab_size]\n",
    "  end_token = [subword_encoder_en.vocab_size + 1]\n",
    "  \n",
    "  # inp_sentence 是字串，我們用 Subword Tokenizer 將其變成子詞的索引序列\n",
    "  # 並在前後加上 BOS / EOS\n",
    "  inp_sentence = start_token + subword_encoder_en.encode(inp_sentence) + end_token\n",
    "  encoder_input = tf.expand_dims(inp_sentence, 0)\n",
    "  \n",
    "  # 跟我們在影片裡看到的一樣，Decoder 在第一個時間點吃進去的輸入\n",
    "  # 是一個只包含一個中文 <start> token 的序列\n",
    "  decoder_input = [subword_encoder_zh.vocab_size]\n",
    "  output = tf.expand_dims(decoder_input, 0)  # 增加 batch 維度\n",
    "  \n",
    "  # auto-regressive，一次生成一個中文字並將預測加到輸入再度餵進 Transformer\n",
    "  for i in range(MAX_LENGTH):\n",
    "    # 每多一個生成的字就得產生新的遮罩\n",
    "    enc_padding_mask, combined_mask, dec_padding_mask = create_masks(\n",
    "        encoder_input, output)\n",
    "  \n",
    "    # predictions.shape == (batch_size, seq_len, vocab_size)\n",
    "    predictions, attention_weights = transformer(encoder_input, \n",
    "                                                 output,\n",
    "                                                 False,\n",
    "                                                 enc_padding_mask,\n",
    "                                                 combined_mask,\n",
    "                                                 dec_padding_mask)\n",
    "    \n",
    "\n",
    "    # 將序列中最後一個 distribution 取出，並將裡頭值最大的當作模型最新的預測字\n",
    "    predictions = predictions[: , -1:, :]  # (batch_size, 1, vocab_size)\n",
    "\n",
    "    predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n",
    "    \n",
    "    # 遇到 <end> token 就停止回傳，代表模型已經產生完結果\n",
    "    if tf.equal(predicted_id, subword_encoder_zh.vocab_size + 1):\n",
    "      return tf.squeeze(output, axis=0), attention_weights\n",
    "    \n",
    "    #將 Transformer 新預測的中文索引加到輸出序列中，讓 Decoder 可以在產生\n",
    "    # 下個中文字的時候關注到最新的 `predicted_id`\n",
    "    output = tf.concat([output, predicted_id], axis=-1)\n",
    "\n",
    "  # 將 batch 的維度去掉後回傳預測的中文索引序列\n",
    "  return tf.squeeze(output, axis=0), attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 要被翻譯的英文句子\n",
    "sentence = \"China, India, and others have enjoyed continuing economic growth.\"\n",
    "\n",
    "# 取得預測的中文索引序列\n",
    "predicted_seq, _ = evaluate(sentence)\n",
    "\n",
    "# 過濾掉 <start> & <end> tokens 並用中文的 subword tokenizer 幫我們將索引序列還原回中文句子\n",
    "target_vocab_size = subword_encoder_zh.vocab_size\n",
    "predicted_seq_without_bos_eos = [idx for idx in predicted_seq if idx < target_vocab_size]\n",
    "predicted_sentence = subword_encoder_zh.decode(predicted_seq_without_bos_eos)\n",
    "\n",
    "print(\"predicted_sentence:\", predicted_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "orig = dict()\n",
    "with open('./translation2019zh/translation2019zh_train.json', 'r', newline='', encoding=\"utf-8\") as file:\n",
    "    data = file.readline()\n",
    "    while data:\n",
    "      orig[json.loads(data)['english']] = json.loads(data)['chinese']\n",
    "      data = file.readline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "en_data = [english for num,english in enumerate(orig, 1)]\n",
    "chi_data = ['\\t' + orig[english] + '\\n' for num,english in enumerate(orig, 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_data = list()\n",
    "chi_data = list()\n",
    "for num,english in enumerate(orig, 1):\n",
    "  if num>60000:\n",
    "    break\n",
    "  else:\n",
    "    en_data.append(english)\n",
    "    chi_data.append('\\t' + orig[english] + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "英文數據:\n",
      " ['For greater sharpness, but with a slight increase in graininess, you can use a 1:1 dilution of this developer.', 'He calls the Green Book, his book of teachings, “the new gospel.', 'And the light breeze moves me to caress her long ear', 'They have the blood of martyrs is the White to flow …', \"Finally, the Lakers head to the Motor City to take on a Pistons team that currently owns the Eastern Conference's second best record (1/31). L.\", '\"The perfect match—my father loves names and Jackie loves money, \" sneered Alexander at the wedding. Neither he nor Christina ever got along with their stepmother17.', 'In 2006, Walmart was charged with racism when its recommendation engine paired Planet of the Apes with a documentary about Martin Luther King.', 'The matte as main copper phase in the cleaning. slag was deter- mined by electron probe microscopic analysis.', 'Have you shined your shoes?', 'The Tanning Matrix can be formed by resorcinol and oxazolidine E, and the reactioncharateristics between Tanning Matrix and collagen were investigated through NMR and size distribution analysis.']\n",
      "\n",
      "中文數據:\n",
      " ['\\t为了更好的锐度，但是附带的会多一些颗粒度，可以使用这个显影剂的1：1稀释液。\\n', '\\t他还把宣扬自己思想的所谓《绿皮书》称作“新福音书”。\\n', '\\t微风推着我去爱抚它的长耳朵\\n', '\\t它们的先烈们的鲜血是白流了…\\n', '\\t最后，在1月31日，湖人将前往汽车城底特律挑战活塞队，活塞近来在东部排名第二。\\n', '\\t“真是天造地设的一对——我父亲喜欢结交名人，杰姬酷爱金钱，”亚历山大在婚礼上讥讽道。他和克里斯蒂娜从未同他们的继母和睦相处过。\\n', '\\t2006年，沃尔玛的推荐引擎竟将《人猿星球》与马丁·路德·金的记录片配成了一对，为此沃尔玛遭到了种族歧视的指控。\\n', '\\t通过电子探针显微分析确定贫化渣中主要铜相为冰铜相。\\n', '\\t吉姆靠给人擦皮鞋为生。\\n', '\\t用甘氨酸模拟胶原，研究间苯二酚-恶唑烷E鞣性基质的形成以及与胶原之间的反应特性。\\n']\n",
      "\n",
      "英文字典:\n",
      " {'造': 0, '扎': 1, 't': 2, '氮': 3, '掉': 4, '肚': 5, '唯': 6, '苗': 7, '姆': 8, '权': 9, '味': 10, '森': 11, '文': 12, '或': 13, '并': 14, '杂': 15, 'k': 16, '渴': 17, '腐': 18, '福': 19, '矩': 20, '正': 21, '窗': 22, '两': 23, '纪': 24, '原': 25, '走': 26, '仆': 27, '善': 28, '悠': 29, '宣': 30, '哑': 31, '表': 32, '县': 33, '―': 34, 'ˉ': 35, '南': 36, '﹑': 37, '写': 38, '夹': 39, '晃': 40, '赋': 41, '亚': 42, '市': 43, '通': 44, '大': 45, '非': 46, '求': 47, '美': 48, '滋': 49, '$': 50, '袋': 51, '准': 52, '时': 53, '很': 54, '乌': 55, '蛮': 56, '’': 57, '8': 58, '￡': 59, '得': 60, '觉': 61, '麻': 62, '吗': 63, '度': 64, '口': 65, '放': 66, '碰': 67, '_': 68, '♠': 69, 'e': 70, '势': 71, '行': 72, '晖': 73, '者': 74, '坚': 75, '号': 76, '类': 77, '池': 78, '岁': 79, '领': 80, '雅': 81, '连': 82, '欣': 83, '%': 84, 'ã': 85, '&': 86, '问': 87, '谈': 88, '品': 89, '队': 90, '倒': 91, '¬': 92, '啊': 93, '漫': 94, '耳': 95, '始': 96, '实': 97, '急': 98, '尝': 99, '失': 100, '巧': 101, 'γ': 102, '生': 103, '钱': 104, '梯': 105, '钥': 106, '扬': 107, '矛': 108, '们': 109, '全': 110, '拐': 111, '祝': 112, '律': 113, '位': 114, '营': 115, '骑': 116, '公': 117, '都': 118, '令': 119, '巴': 120, '宿': 121, '题': 122, '烦': 123, '帮': 124, '许': 125, '科': 126, '包': 127, '凑': 128, '有': 129, '承': 130, 'Y': 131, '会': 132, '洲': 133, '游': 134, '短': 135, '擦': 136, '呈': 137, '由': 138, '阿': 139, '讲': 140, '扶': 141, '电': 142, '剑': 143, '但': 144, '孩': 145, '依': 146, '够': 147, '术': 148, '静': 149, '钩': 150, '扫': 151, '士': 152, '国': 153, '磁': 154, '败': 155, '重': 156, '官': 157, '件': 158, '它': 159, '火': 160, '九': 161, '死': 162, '灰': 163, '硫': 164, '鼎': 165, '哪': 166, '予': 167, '汞': 168, '至': 169, '显': 170, '做': 171, '疲': 172, '木': 173, '3': 174, '撞': 175, '脊': 176, '菲': 177, '则': 178, 'v': 179, '型': 180, '霍': 181, '兰': 182, '乔': 183, '：': 184, '亲': 185, '来': 186, '5': 187, '米': 188, '汁': 189, '斯': 190, '餐': 191, 'o': 192, '´': 193, '皇': 194, '不': 195, '助': 196, '开': 197, '具': 198, '又': 199, '排': 200, 'ú': 201, '他': 202, '称': 203, 'l': 204, '建': 205, '毅': 206, '咳': 207, '磷': 208, 'i': 209, '利': 210, '忙': 211, '免': 212, '谢': 213, '信': 214, '光': 215, '提': 216, '水': 217, '还': 218, '6': 219, '忍': 220, 'W': 221, '最': 222, '匠': 223, '见': 224, '拥': 225, '修': 226, '城': 227, '红': 228, '1': 229, '那': 230, '夫': 231, '\\xa0': 232, '甜': 233, '篇': 234, '博': 235, '您': 236, '虚': 237, 'Ç': 238, 'c': 239, '€': 240, '奥': 241, '清': 242, '�': 243, '作': 244, '团': 245, '著': 246, '篪': 247, '望': 248, '顶': 249, '村': 250, 'ö': 251, '打': 252, '明': 253, '常': 254, '半': 255, '∶': 256, '帝': 257, '程': 258, 'â': 259, '付': 260, '驳': 261, '后': 262, '怒': 263, 'y': 264, '力': 265, '省': 266, '汉': 267, '＄': 268, '继': 269, '辑': 270, '、': 271, '张': 272, '虎': 273, '赛': 274, '尼': 275, '图': 276, '绑': 277, '的': 278, '赏': 279, '荣': 280, '艾': 281, '你': 282, '–': 283, '先': 284, '伯': 285, '子': 286, '切': 287, '西': 288, '深': 289, 'Ñ': 290, '移': 291, '加': 292, '整': 293, '要': 294, '楼': 295, '坪': 296, '>': 297, '当': 298, '置': 299, '臂': 300, '怨': 301, '首': 302, '梅': 303, '⑴': 304, '热': 305, '议': 306, '只': 307, '︰': 308, '台': 309, '黄': 310, '玩': 311, '个': 312, '»': 313, '目': 314, '纳': 315, 'V': 316, '弗': 317, '|': 318, ']': 319, '命': 320, '费': 321, ',': 322, '留': 323, '中': 324, '素': 325, '工': 326, '虫': 327, '夏': 328, '注': 329, 'x': 330, '右': 331, '话': 332, '假': 333, '遍': 334, '察': 335, '么': 336, '格': 337, '色': 338, '艺': 339, '\\\\': 340, '导': 341, '！': 342, '裕': 343, 'μ': 344, '布': 345, '叶': 346, '换': 347, '糖': 348, 'h': 349, '厌': 350, '呢': 351, 'A': 352, '构': 353, '语': 354, '政': 355, '瑞': 356, '曼': 357, '盘': 358, '-': 359, '传': 360, '养': 361, '神': 362, '*': 363, '以': 364, '保': 365, '棕': 366, '石': 367, '耀': 368, '寄': 369, '莓': 370, '今': 371, '漆': 372, '烛': 373, '在': 374, '本': 375, '质': 376, '谁': 377, '多': 378, '）': 379, '沿': 380, '腮': 381, '逊': 382, '比': 383, '巡': 384, '拉': 385, '认': 386, '向': 387, '─': 388, '计': 389, '标': 390, '经': 391, 'F': 392, '供': 393, '除': 394, 'Ü': 395, '玉': 396, '给': 397, 'Q': 398, 'á': 399, 'β': 400, '蛋': 401, '华': 402, '现': 403, '括': 404, 'd': 405, '访': 406, '私': 407, '诉': 408, 'ü': 409, '象': 410, '父': 411, 'δ': 412, 'ê': 413, '习': 414, '》': 415, '净': 416, '再': 417, '接': 418, '份': 419, '服': 420, '狐': 421, '处': 422, '待': 423, '终': 424, '赵': 425, '装': 426, '莱': 427, '¡': 428, '将': 429, '门': 430, '填': 431, '遇': 432, '致': 433, '决': 434, '侧': 435, 'a': 436, '乡': 437, 'u': 438, '里': 439, '醒': 440, '法': 441, '夷': 442, '烧': 443, '肩': 444, '应': 445, '⑤': 446, '才': 447, '欢': 448, '2': 449, '振': 450, '懂': 451, '为': 452, '因': 453, '感': 454, '名': 455, '更': 456, '候': 457, '植': 458, '护': 459, '杀': 460, '，': 461, '下': 462, 'ç': 463, '各': 464, '专': 465, '每': 466, 'b': 467, '杰': 468, '兵': 469, '次': 470, '无': 471, '息': 472, '击': 473, '选': 474, '绿': 475, '请': 476, 'Â': 477, '须': 478, '变': 479, '…': 480, '‘': 481, '抱': 482, '满': 483, '头': 484, '就': 485, '支': 486, '少': 487, '知': 488, '伊': 489, '优': 490, '获': 491, '心': 492, '分': 493, '店': 494, '送': 495, '\"': 496, '年': 497, '险': 498, 'n': 499, '截': 500, '条': 501, '织': 502, '编': 503, '社': 504, '些': 505, '恩': 506, \"'\": 507, '持': 508, '升': 509, '密': 510, '}': 511, '万': 512, '屈': 513, '及': 514, '柄': 515, '界': 516, '冬': 517, '富': 518, '4': 519, '和': 520, '内': 521, '示': 522, '洁': 523, '警': 524, '鄙': 525, '答': 526, ')': 527, '管': 528, '★': 529, '\\xad': 530, 'ó': 531, '算': 532, '泰': 533, '马': 534, '字': 535, '效': 536, '之': 537, '附': 538, '乎': 539, '识': 540, '其': 541, '项': 542, '良': 543, '观': 544, '然': 545, '鸡': 546, '着': 547, '^': 548, '杨': 549, '任': 550, '泉': 551, '太': 552, '节': 553, '与': 554, '党': 555, '奇': 556, '房': 557, '主': 558, '天': 559, '回': 560, '发': 561, '《': 562, '忠': 563, 'w': 564, '身': 565, '野': 566, '油': 567, 'r': 568, '!': 569, '理': 570, '?': 571, '腋': 572, 'K': 573, '拳': 574, '球': 575, '克': 576, '兹': 577, '往': 578, 'B': 579, '没': 580, '域': 581, '<': 582, '→': 583, '禅': 584, '规': 585, '罗': 586, '。': 587, '敬': 588, '边': 589, '吹': 590, 'τ': 591, '翰': 592, '合': 593, '间': 594, '耶': 595, '定': 596, '数': 597, '离': 598, '析': 599, '报': 600, '例': 601, '扔': 602, '灭': 603, '贝': 604, '—': 605, '府': 606, 'X': 607, '随': 608, '民': 609, '~': 610, '左': 611, '吧': 612, '+': 613, '谋': 614, '且': 615, '′': 616, '英': 617, '央': 618, '洛': 619, '义': 620, '进': 621, '取': 622, '证': 623, '{': 624, '层': 625, '陆': 626, '港': 627, 'è': 628, '期': 629, '超': 630, '印': 631, '研': 632, '戏': 633, '≥': 634, '倦': 635, '圆': 636, 'í': 637, '像': 638, '物': 639, '诚': 640, '言': 641, '续': 642, '饮': 643, '肉': 644, '飞': 645, '座': 646, '校': 647, '否': 648, '去': 649, '却': 650, '也': 651, '活': 652, '是': 653, '“': 654, '司': 655, '授': 656, '喷': 657, '组': 658, '含': 659, '元': 660, '解': 661, '样': 662, '·': 663, 'ñ': 664, 's': 665, '#': 666, '腺': 667, '逻': 668, 'L': 669, '祖': 670, '越': 671, '侯': 672, 'E': 673, '？': 674, '章': 675, '镇': 676, '鲁': 677, '种': 678, '书': 679, '句': 680, '技': 681, 'Z': 682, '曾': 683, '成': 684, '志': 685, '«': 686, '儿': 687, '落': 688, '长': 689, '栗': 690, 'T': 691, '好': 692, '拆': 693, '典': 694, '毒': 695, '/': 696, '海': 697, '龟': 698, '妈': 699, '交': 700, '动': 701, '世': 702, '古': 703, '人': 704, 'z': 705, '喊': 706, '记': 707, 'I': 708, '气': 709, '于': 710, '恰': 711, '能': 712, 'D': 713, '武': 714, '催': 715, '用': 716, '易': 717, 'M': 718, '德': 719, '等': 720, 'Á': 721, '暗': 722, '我': 723, '性': 724, '痛': 725, '蜡': 726, '招': 727, 'à': 728, '粉': 729, '远': 730, '步': 731, '钮': 732, '对': 733, '.': 734, ' ': 735, '=': 736, '笑': 737, '髓': 738, '事': 739, '聚': 740, '7': 741, '方': 742, '角': 743, '听': 744, '业': 745, '爱': 746, 'p': 747, '器': 748, '链': 749, '若': 750, '一': 751, '所': 752, ':': 753, 'S': 754, '指': 755, '意': 756, '户': 757, '郡': 758, '勒': 759, '花': 760, '料': 761, '馆': 762, '胞': 763, '疹': 764, '家': 765, '单': 766, '草': 767, '声': 768, '入': 769, '挥': 770, '想': 771, '配': 772, 'P': 773, 'õ': 774, '恭': 775, '序': 776, '”': 777, '直': 778, '举': 779, '资': 780, '疑': 781, 'N': 782, '四': 783, '思': 784, '衣': 785, '9': 786, 'q': 787, '①': 788, '伟': 789, '残': 790, '形': 791, '机': 792, '参': 793, '哲': 794, '即': 795, '她': 796, '这': 797, '据': 798, '体': 799, '男': 800, '约': 801, '根': 802, '柳': 803, '码': 804, '扩': 805, '从': 806, '式': 807, '材': 808, '点': 809, '晚': 810, '转': 811, '验': 812, '自': 813, '微': 814, '妓': 815, '词': 816, '商': 817, 'g': 818, '上': 819, '而': 820, '看': 821, '日': 822, 'ï': 823, '疼': 824, '嗽': 825, '萨': 826, '般': 827, '存': 828, 'O': 829, '踩': 830, '板': 831, '淡': 832, '东': 833, '黑': 834, '育': 835, '戈': 836, '到': 837, '相': 838, '`': 839, '如': 840, '调': 841, '考': 842, '金': 843, '外': 844, '反': 845, '安': 846, '完': 847, '铅': 848, '芽': 849, '地': 850, '情': 851, '™': 852, '过': 853, '况': 854, '控': 855, ';': 856, '剔': 857, '歌': 858, '量': 859, '韦': 860, 'α': 861, '栏': 862, '查': 863, 'R': 864, '空': 865, '黛': 866, '操': 867, '早': 868, '什': 869, '场': 870, '室': 871, '员': 872, '值': 873, '△': 874, '尔': 875, '酒': 876, '普': 877, '果': 878, '学': 879, '智': 880, '星': 881, '脑': 882, 'U': 883, '汗': 884, '小': 885, '(': 886, '坦': 887, '棒': 888, '按': 889, '阵': 890, '炎': 891, '旧': 892, '树': 893, '隆': 894, '娜': 895, 'm': 896, '音': 897, '另': 898, '差': 899, '仅': 900, '总': 901, '挑': 902, '把': 903, '便': 904, 'G': 905, '说': 906, '毕': 907, '奉': 908, '朗': 909, '达': 910, '库': 911, '@': 912, '已': 913, '必': 914, '食': 915, '己': 916, '基': 917, '促': 918, 'f': 919, '何': 920, '玛': 921, 'J': 922, '◎': 923, '老': 924, 'j': 925, '前': 926, '暴': 927, '伦': 928, '页': 929, 'ä': 930, 'ô': 931, 'Ⅱ': 932, '使': 933, '笔': 934, '该': 935, '女': 936, '疫': 937, '教': 938, '同': 939, '】': 940, '备': 941, '未': 942, '历': 943, '眼': 944, 'C': 945, '几': 946, '代': 947, '压': 948, '精': 949, '道': 950, '晨': 951, '；': 952, '白': 953, 'H': 954, '细': 955, '价': 956, '真': 957, '高': 958, '弘': 959, '病': 960, '被': 961, '告': 962, '蓝': 963, '可': 964, '叫': 965, '投': 966, 'Ö': 967, '祭': 968, '0': 969, '洗': 970, '究': 971, '试': 972, '仍': 973, '缺': 974, '刻': 975, '检': 976, '出': 977, '京': 978, '区': 979, '菜': 980, '旋': 981, '借': 982, '嘴': 983, '部': 984, 'É': 985, '制': 986, '了': 987, '面': 988, '设': 989, '[': 990, 'é': 991, '℃': 992, '呼': 993, '起': 994, '展': 995, '风': 996, '香': 997, '手': 998, '化': 999, '（': 1000}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "中文字典共計\n",
      ": {'摩': 0, '纲': 1, '味': 2, '院': 3, '视': 4, '潺': 5, '披': 6, '写': 7, '簬': 8, '燮': 9, '淬': 10, '觉': 11, '放': 12, '势': 13, '淩': 14, '膳': 15, '毫': 16, '蜃': 17, '巷': 18, '戕': 19, '墓': 20, '耳': 21, '熊': 22, '烬': 23, '俩': 24, '˙': 25, '凝': 26, '轲': 27, '科': 28, '沓': 29, '诲': 30, '国': 31, '橘': 32, '椒': 33, '灰': 34, '佃': 35, '桃': 36, '酚': 37, '皒': 38, '汝': 39, '羧': 40, '攞': 41, '坑': 42, '乔': 43, '拚': 44, '柠': 45, '镜': 46, '嘈': 47, '广': 48, '荧': 49, '玑': 50, '馋': 51, '稽': 52, '苯': 53, '衢': 54, '竹': 55, '圈': 56, '肿': 57, '测': 58, '诈': 59, '夫': 60, '母': 61, '遭': 62, '担': 63, '徜': 64, '钠': 65, '憾': 66, '抒': 67, '轰': 68, '唤': 69, '囡': 70, '继': 71, '罢': 72, '抵': 73, '罔': 74, '泾': 75, '塔': 76, '子': 77, '楞': 78, '雦': 79, '楼': 80, '滚': 81, '焦': 82, '纡': 83, '稔': 84, '命': 85, '吋': 86, '昭': 87, '蕃': 88, '番': 89, '贫': 90, '擀': 91, '莪': 92, '构': 93, '版': 94, '邻': 95, '欠': 96, '阅': 97, '堵': 98, '林': 99, '躏': 100, '嵩': 101, '降': 102, '今': 103, '本': 104, '墟': 105, '豺': 106, '練': 107, '屋': 108, '庶': 109, '猫': 110, '惟': 111, '龅': 112, '鹌': 113, '拦': 114, '缆': 115, '蹬': 116, '篑': 117, '挤': 118, '栋': 119, '灏': 120, '诉': 121, '箭': 122, '臌': 123, '逐': 124, '淇': 125, '襁': 126, '珥': 127, '莱': 128, '薯': 129, '揾': 130, '凿': 131, '狮': 132, '肩': 133, '拙': 134, '唇': 135, '迅': 136, '辰': 137, '铤': 138, '堪': 139, '每': 140, '潴': 141, '赃': 142, '豹': 143, '蒐': 144, '阈': 145, '嘢': 146, '‘': 147, '插': 148, '濮': 149, '崭': 150, '险': 151, '狠': 152, '吲': 153, '颢': 154, '抬': 155, '富': 156, '曰': 157, '偎': 158, '翳': 159, '稻': 160, '昉': 161, '吉': 162, '麽': 163, '遨': 164, '锈': 165, '良': 166, '痪': 167, '隃': 168, '噪': 169, '俚': 170, '鹞': 171, '垩': 172, '魂': 173, '跤': 174, '饿': 175, '改': 176, '钬': 177, '靠': 178, '伽': 179, '涵': 180, '粮': 181, '淤': 182, '姨': 183, '锘': 184, '喹': 185, '亽': 186, '苛': 187, '湟': 188, '塞': 189, '锁': 190, '娟': 191, '敝': 192, '胍': 193, '吠': 194, '弧': 195, '恳': 196, '淀': 197, '盂': 198, '吱': 199, '逃': 200, '剧': 201, '俐': 202, '诚': 203, '吒': 204, '啾': 205, '牟': 206, '渭': 207, '筹': 208, '洙': 209, '祖': 210, '范': 211, '酐': 212, '/': 213, '邸': 214, '锃': 215, '动': 216, '康': 217, '叙': 218, '硐': 219, '夙': 220, '奈': 221, '痛': 222, '憨': 223, '煅': 224, '月': 225, '棪': 226, '镂': 227, '陵': 228, '耍': 229, '仕': 230, '杉': 231, '攸': 232, '泪': 233, '垢': 234, '莨': 235, '娠': 236, '矣': 237, '衣': 238, '赜': 239, '残': 240, '绊': 241, '辗': 242, '即': 243, '她': 244, '芡': 245, '甩': 246, '煳': 247, '輪': 248, '钺': 249, '栎': 250, '微': 251, '弦': 252, '聘': 253, '裑': 254, '盟': 255, '裱': 256, '沟': 257, '鳔': 258, '饷': 259, '完': 260, '率': 261, '券': 262, '桑': 263, '栏': 264, '醛': 265, '绰': 266, '亩': 267, '值': 268, '忏': 269, '脑': 270, '嘭': 271, '抉': 272, '厝': 273, '蝴': 274, '雎': 275, '缐': 276, '逢': 277, '胬': 278, '靡': 279, '铟': 280, '蹴': 281, '另': 282, '挟': 283, '闰': 284, '鳌': 285, '蚣': 286, '姑': 287, '备': 288, '薇': 289, '磅': 290, '鲷': 291, '眼': 292, '毎': 293, '啤': 294, '哨': 295, '霏': 296, '醚': 297, '0': 298, '既': 299, '产': 300, '囫': 301, '聋': 302, '厍': 303, '困': 304, '锶': 305, '墒': 306, '（': 307, '涡': 308, '漳': 309, '虏': 310, '并': 311, '毡': 312, '巙': 313, '搀': 314, '牺': 315, '走': 316, '埠': 317, '鲆': 318, '榴': 319, '睿': 320, '非': 321, '8': 322, '畅': 323, '_': 324, '谎': 325, '琅': 326, '翎': 327, '峻': 328, '硅': 329, '菊': 330, '贩': 331, '炸': 332, '耙': 333, '谱': 334, '隔': 335, '馏': 336, '羙': 337, '廖': 338, '乸': 339, '许': 340, '屠': 341, '螂': 342, '〕': 343, '襄': 344, '拗': 345, '遮': 346, '攻': 347, '谚': 348, '居': 349, '铲': 350, '它': 351, '瓦': 352, '獭': 353, '昙': 354, '予': 355, '聍': 356, '遐': 357, '鸳': 358, '则': 359, '型': 360, '兰': 361, '愣': 362, '膀': 363, '腾': 364, '颌': 365, '蓦': 366, '戮': 367, '褂': 368, '疯': 369, '余': 370, '瞠': 371, '枕': 372, '毅': 373, '枢': 374, '愁': 375, '颞': 376, '钅': 377, '那': 378, '玟': 379, '焉': 380, '粳': 381, '袄': 382, '碓': 383, '团': 384, '望': 385, '逾': 386, '榚': 387, '觎': 388, '妆': 389, '渲': 390, '喧': 391, '阮': 392, '厢': 393, '霜': 394, '唬': 395, '毁': 396, '嘉': 397, '箧': 398, '愫': 399, '嚏': 400, '扰': 401, '肽': 402, '梅': 403, '嫉': 404, '踪': 405, '搂': 406, '姻': 407, '塘': 408, '稿': 409, '忻': 410, '鬃': 411, '敦': 412, '皎': 413, '沧': 414, '保': 415, '迩': 416, '石': 417, '鳟': 418, '荠': 419, '跷': 420, '丕': 421, '咧': 422, '狸': 423, '多': 424, '屿': 425, '砙': 426, '馞': 427, '孛': 428, '轨': 429, '肇': 430, '寅': 431, '唱': 432, '侦': 433, '经': 434, '驹': 435, '虺': 436, '婢': 437, '隽': 438, '渡': 439, '＜': 440, '宴': 441, '钞': 442, '椿': 443, '陋': 444, '谦': 445, '里': 446, '豚': 447, '绽': 448, '铣': 449, '胫': 450, '铸': 451, '2': 452, '玲': 453, '赁': 454, '锭': 455, '蹂': 456, '％': 457, '忖': 458, '邬': 459, '狙': 460, '戟': 461, '丁': 462, '刍': 463, '褥': 464, '皓': 465, '慢': 466, '裤': 467, '値': 468, '涛': 469, '崀': 470, '癜': 471, '缎': 472, '恢': 473, '蚪': 474, '肪': 475, '梆': 476, '敏': 477, '循': 478, '^': 479, '滑': 480, '粿': 481, '贮': 482, '天': 483, '樟': 484, '儡': 485, '巅': 486, '翅': 487, '!': 488, '爻': 489, '歹': 490, '穿': 491, '迳': 492, '邑': 493, '襟': 494, '枷': 495, '宫': 496, '菠': 497, '结': 498, '讧': 499, '誉': 500, '币': 501, '『': 502, '敬': 503, '嗯': 504, '捣': 505, '屑': 506, '析': 507, '酵': 508, '乍': 509, '班': 510, '洛': 511, '皮': 512, '搏': 513, '氨': 514, '黒': 515, '断': 516, '冗': 517, '续': 518, '泣': 519, '喳': 520, '婪': 521, '朋': 522, '训': 523, '漾': 524, '嘶': 525, '儒': 526, '甬': 527, '迤': 528, '鲁': 529, '雇': 530, '君': 531, '蝶': 532, '檀': 533, '曾': 534, '志': 535, '凛': 536, '叔': 537, '痴': 538, '闷': 539, '世': 540, '贰': 541, '冯': 542, '嘿': 543, '岂': 544, '谄': 545, '逼': 546, '羚': 547, '秤': 548, '辉': 549, '鲭': 550, '磕': 551, '审': 552, '鲼': 553, '.': 554, '诩': 555, '蹶': 556, '慨': 557, '雕': 558, '槌': 559, '嗔': 560, '糕': 561, '抢': 562, '虹': 563, '缉': 564, '狒': 565, '锕': 566, '客': 567, '铈': 568, '伟': 569, '胳': 570, '咱': 571, '材': 572, '揉': 573, '点': 574, '窒': 575, '焘': 576, '冝': 577, 'O': 578, '椽': 579, '孝': 580, '绢': 581, '砥': 582, '地': 583, '况': 584, '拨': 585, '旻': 586, '场': 587, '哗': 588, '辔': 589, '蔻': 590, '趋': 591, '嗷': 592, '栌': 593, '蔬': 594, '瑢': 595, '佘': 596, '臭': 597, '螫': 598, '垄': 599, '奉': 600, '觧': 601, '裔': 602, '玛': 603, '赀': 604, '偾': 605, '庾': 606, '焚': 607, '僧': 608, '珞': 609, '衮': 610, '弘': 611, '∙': 612, '缩': 613, '唿': 614, '祭': 615, '芍': 616, '狩': 617, '抗': 618, '惧': 619, '厕': 620, '鳃': 621, '龄': 622, '借': 623, '嚼': 624, '龋': 625, '捌': 626, '霹': 627, '崛': 628, '买': 629, '呼': 630, '豔': 631, '婆': 632, '戛': 633, '粹': 634, '陇': 635, '垭': 636, '梓': 637, '袭': 638, '率': 639, '拼': 640, '碌': 641, '俟': 642, '羊': 643, '仆': 644, '表': 645, '磴': 646, '缇': 647, '抨': 648, '通': 649, '荤': 650, '肤': 651, '粪': 652, '肟': 653, '害': 654, '者': 655, '楷': 656, '射': 657, '震': 658, '愍': 659, '谈': 660, '尬': 661, '絮': 662, '悍': 663, '始': 664, '莹': 665, '砍': 666, '嗓': 667, '鑑': 668, '吡': 669, '革': 670, '禺': 671, '们': 672, '慰': 673, '叁': 674, '爆': 675, '窝': 676, '封': 677, '娩': 678, '樓': 679, '闵': 680, '斐': 681, '汽': 682, '念': 683, '洋': 684, '电': 685, '宵': 686, '罨': 687, '磨': 688, '抖': 689, '揍': 690, '挂': 691, '炖': 692, '吞': 693, '耷': 694, '泼': 695, '坝': 696, '开': 697, '彷': 698, 'ú': 699, '悟': 700, '纫': 701, '唠': 702, '羰': 703, '谢': 704, '锡': 705, '托': 706, '屎': 707, '瘪': 708, '诂': 709, '消': 710, 'W': 711, '痂': 712, '见': 713, '雌': 714, '甜': 715, '喁': 716, '峒': 717, '坳': 718, '暇': 719, '稍': 720, '篪': 721, '舌': 722, '卜': 723, '蜂': 724, '驳': 725, '崤': 726, '犀': 727, '洩': 728, '靴': 729, '灵': 730, '芦': 731, '抠': 732, '鹉': 733, '埸': 734, '索': 735, '閒': 736, '新': 737, '宏': 738, '辍': 739, '寺': 740, '鳄': 741, '笙': 742, '戚': 743, '>': 744, '僳': 745, '⑴': 746, '擎': 747, '舶': 748, '缠': 749, '零': 750, '畦': 751, ']': 752, '练': 753, '素': 754, '寥': 755, '争': 756, '话': 757, '羲': 758, '格': 759, '俳': 760, '燧': 761, '政': 762, '捻': 763, '圣': 764, '拿': 765, '忐': 766, '鍊': 767, '拉': 768, '痈': 769, '婿': 770, 'Q': 771, '涨': 772, '捍': 773, '访': 774, '潸': 775, '缀': 776, '鄂': 777, '傻': 778, '锨': 779, '衷': 780, '致': 781, '⒋': 782, '决': 783, '末': 784, '贞': 785, '噜': 786, '槲': 787, '馈': 788, '惫': 789, '卟': 790, '螬': 791, '匆': 792, '枋': 793, '护': 794, '钦': 795, '血': 796, 'b': 797, '冰': 798, '募': 799, '呻': 800, '甲': 801, '抱': 802, '褶': 803, '舍': 804, '少': 805, '知': 806, '分': 807, '蛴': 808, '豫': 809, '孺': 810, '赫': 811, '剡': 812, '琦': 813, '皋': 814, '}': 815, '介': 816, '及': 817, '锛': 818, '饼': 819, '硎': 820, '示': 821, '桡': 822, '淖': 823, '字': 824, '谂': 825, '烈': 826, '恼': 827, '隋': 828, '魔': 829, '悒': 830, '酋': 831, '诠': 832, '￥': 833, '收': 834, '菈': 835, '陕': 836, '螈': 837, '污': 838, '朦': 839, '鹫': 840, '拈': 841, '→': 842, '腼': 843, '疆': 844, '煲': 845, '膊': 846, '沬': 847, '集': 848, '逹': 849, '边': 850, '犒': 851, '肠': 852, '骡': 853, '論': 854, '耶': 855, '闾': 856, '随': 857, '颤': 858, '壶': 859, '左': 860, '积': 861, '谤': 862, '渃': 863, '宪': 864, '阒': 865, '津': 866, '印': 867, '琥': 868, '晟': 869, '粞': 870, '爽': 871, '菱': 872, '乃': 873, '蹲': 874, '冽': 875, '校': 876, '嘀': 877, '蛛': 878, '网': 879, '措': 880, '鹳': 881, '茹': 882, 'ñ': 883, '啰': 884, '章': 885, '书': 886, '旗': 887, '覸': 888, '咐': 889, '酷': 890, '胂': 891, '翯': 892, '闆': 893, '纷': 894, '缤': 895, '核': 896, '苜': 897, '等': 898, '汾': 899, '我': 900, '彭': 901, '粉': 902, '匹': 903, '呷': 904, '镦': 905, '惆': 906, 'p': 907, '灯': 908, '刀': 909, '杜': 910, '傍': 911, '蒴': 912, '胞': 913, '驴': 914, '睛': 915, '秦': 916, '声': 917, '枭': 918, '淑': 919, '迍': 920, '钻': 921, '纂': 922, '臾': 923, '蛟': 924, '眉': 925, '玢': 926, '帖': 927, '属': 928, '铨': 929, '茏': 930, '刨': 931, '拭': 932, '彼': 933, '锇': 934, '看': 935, '堇': 936, '枉': 937, '倚': 938, '申': 939, '廪': 940, '魏': 941, '帽': 942, '牵': 943, '郯': 944, '猴': 945, '晓': 946, 'α': 947, '祜': 948, '胭': 949, '空': 950, '早': 951, '什': 952, '础': 953, '疵': 954, '辩': 955, '紧': 956, '蛰': 957, '学': 958, '咸': 959, '讨': 960, '坦': 961, '兊': 962, '仪': 963, '赦': 964, '蝣': 965, '伞': 966, '烩': 967, '挑': 968, '说': 969, '蚌': 970, '蛇': 971, '挝': 972, '使': 973, '笔': 974, '碱': 975, '跳': 976, 'C': 977, '蜘': 978, '压': 979, '篁': 980, '铋': 981, '烁': 982, '嫁': 983, '被': 984, '痕': 985, '利': 986, '迸': 987, '淞': 988, '亦': 989, '昊': 990, '钕': 991, '骠': 992, '浔': 993, '咦': 994, '迁': 995, '煮': 996, '第': 997, '樱': 998, '珑': 999, '嘻': 1000, '北': 1001, '鎏': 1002, '伪': 1003, '两': 1004, '灿': 1005, '槽': 1006, '原': 1007, '搁': 1008, '雀': 1009, '鸾': 1010, '亘': 1011, '哑': 1012, '夹': 1013, '饲': 1014, '奶': 1015, '岢': 1016, '桌': 1017, '拽': 1018, '温': 1019, '口': 1020, '狈': 1021, '羯': 1022, '惩': 1023, '恸': 1024, '唷': 1025, '益': 1026, '溃': 1027, '鉴': 1028, '生': 1029, '扬': 1030, '顺': 1031, '沉': 1032, '镶': 1033, '氡': 1034, '茉': 1035, '掮': 1036, '峄': 1037, '跑': 1038, '皙': 1039, '敷': 1040, '潍': 1041, '硫': 1042, '臀': 1043, '嗝': 1044, '媳': 1045, '吨': 1046, '徐': 1047, '邛': 1048, '玫': 1049, '冀': 1050, '不': 1051, '皇': 1052, '眠': 1053, '弯': 1054, '堤': 1055, '淙': 1056, '璻': 1057, '饪': 1058, '光': 1059, '拥': 1060, '棣': 1061, '宛': 1062, '魯': 1063, '苦': 1064, '瞅': 1065, '清': 1066, '娴': 1067, '噔': 1068, '嬉': 1069, '饯': 1070, '垮': 1071, '省': 1072, '荨': 1073, '赏': 1074, '昂': 1075, '峇': 1076, '咿': 1077, '懮': 1078, '柜': 1079, '撑': 1080, '只': 1081, '昏': 1082, '拱': 1083, '沅': 1084, '工': 1085, '徙': 1086, '注': 1087, '径': 1088, '瑕': 1089, '鸮': 1090, '烂': 1091, '盖': 1092, '唉': 1093, '◇': 1094, '摄': 1095, '*': 1096, '骇': 1097, '毂': 1098, '藐': 1099, '嫡': 1100, '扛': 1101, '､': 1102, '壕': 1103, '泓': 1104, '标': 1105, '揪': 1106, '栩': 1107, '壮': 1108, '给': 1109, '萸': 1110, 'd': 1111, '挖': 1112, '瞻': 1113, '爷': 1114, '腿': 1115, '眯': 1116, '牢': 1117, '装': 1118, '绳': 1119, '耕': 1120, '斋': 1121, '獠': 1122, '映': 1123, '铠': 1124, '蔼': 1125, '耗': 1126, '霸': 1127, '摸': 1128, '妹': 1129, '淌': 1130, '植': 1131, '旬': 1132, '杀': 1133, '缮': 1134, '晒': 1135, '筋': 1136, '请': 1137, '堑': 1138, '俑': 1139, '燎': 1140, '头': 1141, '心': 1142, '尚': 1143, '编': 1144, '例': 1145, '窨': 1146, '错': 1147, '碾': 1148, '镊': 1149, '鳍': 1150, '赖': 1151, '绩': 1152, '切': 1153, '洁': 1154, '鄙': 1155, '孢': 1156, '蜗': 1157, '柯': 1158, '马': 1159, '井': 1160, '噬': 1161, '吆': 1162, '漏': 1163, '熬': 1164, '镀': 1165, '缥': 1166, 'r': 1167, '理': 1168, '惕': 1169, 'K': 1170, '婵': 1171, '克': 1172, '浃': 1173, '没': 1174, '肝': 1175, '篱': 1176, '误': 1177, '崇': 1178, '浣': 1179, '侃': 1180, '肆': 1181, '盥': 1182, '枝': 1183, '萃': 1184, '荸': 1185, '髦': 1186, 'X': 1187, '+': 1188, '庚': 1189, '炕': 1190, '摊': 1191, '巩': 1192, '赔': 1193, '否': 1194, '湄': 1195, '吓': 1196, '“': 1197, '耻': 1198, '毋': 1199, '烽': 1200, '砧': 1201, '种': 1202, '祈': 1203, '落': 1204, '栗': 1205, '蚯': 1206, '熏': 1207, '禹': 1208, '涂': 1209, '摞': 1210, '杏': 1211, '藜': 1212, '催': 1213, '眙': 1214, '锑': 1215, '咭': 1216, '酪': 1217, '癎': 1218, '豉': 1219, '蜡': 1220, '仡': 1221, '婷': 1222, '膂': 1223, '戍': 1224, '坐': 1225, ' ': 1226, '充': 1227, '掐': 1228, '髓': 1229, '器': 1230, '复': 1231, '艰': 1232, '嚷': 1233, '茗': 1234, '廓': 1235, '觐': 1236, '锄': 1237, '兢': 1238, '煎': 1239, '勋': 1240, '眩': 1241, '袐': 1242, '厄': 1243, '眶': 1244, '涌': 1245, '疑': 1246, 'N': 1247, '颚': 1248, '参': 1249, '螳': 1250, '榆': 1251, '懒': 1252, '自': 1253, '愧': 1254, '屹': 1255, '存': 1256, '罚': 1257, '相': 1258, '窍': 1259, '彩': 1260, '浙': 1261, '安': 1262, '趟': 1263, '蔷': 1264, '惭': 1265, '辞': 1266, '庆': 1267, '慈': 1268, '糠': 1269, '永': 1270, '湿': 1271, '室': 1272, '锯': 1273, '１': 1274, '果': 1275, '架': 1276, '小': 1277, '阴': 1278, '埚': 1279, '纸': 1280, '旧': 1281, '笠': 1282, '稃': 1283, '枣': 1284, '拴': 1285, '磬': 1286, '侨': 1287, '歪': 1288, '瞳': 1289, '蔓': 1290, '涩': 1291, '逛': 1292, '遛': 1293, '纱': 1294, '菘': 1295, '娅': 1296, '甄': 1297, '蜀': 1298, '宗': 1299, '碍': 1300, '高': 1301, '浅': 1302, '剃': 1303, '獾': 1304, '撤': 1305, '哇': 1306, '觇': 1307, '蛭': 1308, '铜': 1309, '挞': 1310, '区': 1311, '芭': 1312, '纠': 1313, '兆': 1314, '部': 1315, '勇': 1316, '嶙': 1317, '姬': 1318, '起': 1319, '琐': 1320, '夭': 1321, '吼': 1322, '砸': 1323, '姆': 1324, '柿': 1325, '褚': 1326, '纨': 1327, '飓': 1328, '鸿': 1329, '燕': 1330, '喻': 1331, '晷': 1332, '赋': 1333, '沭': 1334, '段': 1335, '搡': 1336, '吻': 1337, '辣': 1338, '茸': 1339, '融': 1340, '连': 1341, '佢': 1342, '憩': 1343, '囵': 1344, '佟': 1345, '庐': 1346, '窑': 1347, '锻': 1348, '簸': 1349, '拄': 1350, '位': 1351, '瑟': 1352, '疽': 1353, '丑': 1354, '猗': 1355, '氙': 1356, '减': 1357, '擘': 1358, '擦': 1359, '扶': 1360, '阿': 1361, '哩': 1362, '啪': 1363, '貂': 1364, '迥': 1365, '扫': 1366, '邢': 1367, '低': 1368, '愉': 1369, '木': 1370, '菲': 1371, '诸': 1372, '扑': 1373, '5': 1374, '迷': 1375, '蹦': 1376, '椋': 1377, '頫': 1378, '银': 1379, '似': 1380, '橄': 1381, '跪': 1382, '提': 1383, '禄': 1384, '秀': 1385, '忍': 1386, '棵': 1387, '崖': 1388, '岑': 1389, '榔': 1390, '馁': 1391, '叴': 1392, '�': 1393, '．': 1394, 'η': 1395, '哟': 1396, '仁': 1397, '航': 1398, '勃': 1399, '侣': 1400, '搬': 1401, '柴': 1402, '帝': 1403, '獒': 1404, '力': 1405, '躁': 1406, '嘱': 1407, '掳': 1408, '夤': 1409, '奴': 1410, '睹': 1411, '脂': 1412, '纾': 1413, '讽': 1414, '妊': 1415, '估': 1416, '驱': 1417, '刷': 1418, '葜': 1419, '徒': 1420, '膏': 1421, '毯': 1422, '毗': 1423, '衔': 1424, '怀': 1425, '绝': 1426, '浦': 1427, '羹': 1428, '茫': 1429, '距': 1430, '褡': 1431, '菂': 1432, '卖': 1433, '噶': 1434, '佰': 1435, '床': 1436, 'h': 1437, '筵': 1438, '睦': 1439, '婴': 1440, '湘': 1441, '肯': 1442, '掱': 1443, '窥': 1444, '嫔': 1445, '箴': 1446, '涎': 1447, '玉': 1448, '乖': 1449, '处': 1450, '哥': 1451, '辆': 1452, '菝': 1453, '欢': 1454, '溉': 1455, '鳅': 1456, '歧': 1457, '淋': 1458, '餮': 1459, '境': 1460, '媾': 1461, '培': 1462, '赣': 1463, '兵': 1464, '变': 1465, '窿': 1466, '诟': 1467, '豆': 1468, '禾': 1469, '倘': 1470, '锗': 1471, '珂': 1472, '喃': 1473, '璀': 1474, '褊': 1475, '屈': 1476, '忱': 1477, '臜': 1478, '答': 1479, '噩': 1480, '\\u2009': 1481, '妨': 1482, '绔': 1483, '删': 1484, '薪': 1485, '绎': 1486, '姐': 1487, '骸': 1488, '悻': 1489, '仑': 1490, '野': 1491, '笺': 1492, '烹': 1493, '簧': 1494, '硒': 1495, '帘': 1496, '栈': 1497, '暴': 1498, '颦': 1499, '届': 1500, '溴': 1501, '汰': 1502, '斓': 1503, '佣': 1504, '泗': 1505, '贝': 1506, '吏': 1507, '鸽': 1508, '蒋': 1509, '佬': 1510, '淮': 1511, '玄': 1512, '晔': 1513, '踽': 1514, '尺': 1515, '戏': 1516, '爪': 1517, '却': 1518, '倔': 1519, '简': 1520, '芸': 1521, '紊': 1522, '傥': 1523, '休': 1524, '贲': 1525, '漕': 1526, '惶': 1527, '饨': 1528, '粱': 1529, '凭': 1530, '夸': 1531, '匀': 1532, '壑': 1533, '«': 1534, '涤': 1535, '好': 1536, '滩': 1537, '典': 1538, '砾': 1539, '楔': 1540, 'z': 1541, '螺': 1542, '喊': 1543, '咩': 1544, '锥': 1545, '暗': 1546, '車': 1547, '事': 1548, '荞': 1549, '业': 1550, '捺': 1551, '岘': 1552, '兑': 1553, '迲': 1554, '黝': 1555, '咬': 1556, '槛': 1557, '山': 1558, '单': 1559, 'P': 1560, '舜': 1561, '资': 1562, '赶': 1563, '闫': 1564, '缘': 1565, '趿': 1566, '赉': 1567, '鸣': 1568, '形': 1569, '钎': 1570, '幺': 1571, '鹧': 1572, '阪': 1573, '霭': 1574, '溺': 1575, '喋': 1576, '鞣': 1577, '癫': 1578, '锰': 1579, '踩': 1580, '桶': 1581, '疮': 1582, '徘': 1583, '庵': 1584, '亮': 1585, '贬': 1586, '泯': 1587, '殆': 1588, '锟': 1589, '绌': 1590, '荇': 1591, '喔': 1592, '外': 1593, '靼': 1594, '旷': 1595, '迫': 1596, '氏': 1597, '斑': 1598, 'R': 1599, '乓': 1600, '燊': 1601, '瞒': 1602, '嗳': 1603, '厚': 1604, '顷': 1605, '鳞': 1606, '星': 1607, '侮': 1608, '跌': 1609, '铍': 1610, '芬': 1611, 'ω': 1612, '彻': 1613, '朵': 1614, '潇': 1615, '仅': 1616, '挣': 1617, '殡': 1618, '婀': 1619, '兩': 1620, '聪': 1621, '繸': 1622, '郁': 1623, '奠': 1624, '教': 1625, '哄': 1626, '鲈': 1627, '孬': 1628, '桢': 1629, '精': 1630, '醇': 1631, '辈': 1632, '董': 1633, '白': 1634, '细': 1635, '猩': 1636, '卷': 1637, '拌': 1638, '恕': 1639, 'Ö': 1640, '松': 1641, '凹': 1642, '噎': 1643, '璇': 1644, '蛀': 1645, '堂': 1646, '钙': 1647, '立': 1648, '蔽': 1649, '贷': 1650, '奏': 1651, '铀': 1652, '泄': 1653, '[': 1654, '芋': 1655, '棋': 1656, '鲔': 1657, '羽': 1658, 't': 1659, '仓': 1660, '囊': 1661, '鹅': 1662, '肥': 1663, '渴': 1664, '颖': 1665, '卵': 1666, '殉': 1667, '鹏': 1668, '坩': 1669, '―': 1670, '伍': 1671, '簿': 1672, '求': 1673, '姜': 1674, '乌': 1675, '奭': 1676, '猎': 1677, '六': 1678, '祛': 1679, '炒': 1680, '怵': 1681, '谑': 1682, '瘢': 1683, '串': 1684, '荼': 1685, '捆': 1686, '笞': 1687, '潮': 1688, '辫': 1689, '滔': 1690, '匪': 1691, '拐': 1692, '烟': 1693, '赌': 1694, '帮': 1695, '芙': 1696, '颁': 1697, '譬': 1698, '嗤': 1699, 'Y': 1700, '猖': 1701, '咻': 1702, '葡': 1703, '绻': 1704, '蛆': 1705, '恤': 1706, '洐': 1707, '榛': 1708, '重': 1709, '官': 1710, '横': 1711, '念': 1712, '鼎': 1713, '跄': 1714, '突': 1715, '郸': 1716, '圃': 1717, '簇': 1718, '轻': 1719, '撞': 1720, '沃': 1721, '凉': 1722, '铰': 1723, '亲': 1724, '米': 1725, '撵': 1726, '临': 1727, '筐': 1728, '饺': 1729, '砂': 1730, '咒': 1731, '寂': 1732, '衅': 1733, 'i': 1734, '芳': 1735, '碟': 1736, '箐': 1737, '笕': 1738, '疗': 1739, '斡': 1740, '呑': 1741, '俊': 1742, '咆': 1743, '损': 1744, '林': 1745, '染': 1746, '堆': 1747, '【': 1748, '楑': 1749, '军': 1750, '杯': 1751, '懋': 1752, '顶': 1753, '久': 1754, '跺': 1755, '囿': 1756, '郎': 1757, '程': 1758, '桩': 1759, '∶': 1760, '史': 1761, '菪': 1762, '模': 1763, '汉': 1764, '哀': 1765, '尴': 1766, '摧': 1767, '黧': 1768, '冤': 1769, '暂': 1770, '盔': 1771, '西': 1772, '馒': 1773, '傣': 1774, '移': 1775, '啧': 1776, '阉': 1777, '坪': 1778, '当': 1779, '讣': 1780, '胜': 1781, '论': 1782, '窘': 1783, '瓜': 1784, '睡': 1785, '呵': 1786, '莴': 1787, '劫': 1788, '贿': 1789, '峦': 1790, '虫': 1791, '顾': 1792, '璜': 1793, '溯': 1794, '奂': 1795, '色': 1796, '\\\\': 1797, '嗾': 1798, '田': 1799, '腌': 1800, '罪': 1801, '邀': 1802, '燔': 1803, '涉': 1804, '账': 1805, '髅': 1806, '蝾': 1807, '浉': 1808, '沿': 1809, '擒': 1810, '怯': 1811, '筷': 1812, '舷': 1813, '夯': 1814, '沛': 1815, '擤': 1816, '鸠': 1817, '龃': 1818, '麦': 1819, '臨': 1820, '终': 1821, '啲': 1822, '赓': 1823, '轴': 1824, '晕': 1825, '都': 1826, '暨': 1827, '纽': 1828, '胀': 1829, '阐': 1830, '沽': 1831, '专': 1832, '瑁': 1833, '绯': 1834, '犸': 1835, '忒': 1836, '斤': 1837, '芪': 1838, '锝': 1839, '殚': 1840, '确': 1841, '岸': 1842, '沐': 1843, 'n': 1844, '织': 1845, '扣': 1846, '癌': 1847, '密': 1848, '梧': 1849, '鸥': 1850, '涝': 1851, '腻': 1852, '剐': 1853, '柄': 1854, '俾': 1855, '喓': 1856, ')': 1857, '采': 1858, '瞟': 1859, '恐': 1860, '茴': 1861, '漱': 1862, '扯': 1863, '鬈': 1864, '棠': 1865, '奔': 1866, '身': 1867, '訏': 1868, '釉': 1869, '泅': 1870, '斟': 1871, '族': 1872, '拳': 1873, '鲶': 1874, '略': 1875, '萜': 1876, '蒽': 1877, '冢': 1878, '雷': 1879, '铧': 1880, '踢': 1881, '坯': 1882, '貌': 1883, '协': 1884, '虽': 1885, '间': 1886, '离': 1887, '灭': 1888, '傲': 1889, '拧': 1890, '灞': 1891, '淄': 1892, '腓': 1893, '戾': 1894, '見': 1895, '荀': 1896, '廉': 1897, '剽': 1898, '棚': 1899, '盐': 1900, '苣': 1901, '膨': 1902, '④': 1903, '凯': 1904, '～': 1905, '衩': 1906, '飕': 1907, '寒': 1908, '册': 1909, '玺': 1910, '朔': 1911, '扭': 1912, '丣': 1913, '成': 1914, '峨': 1915, '长': 1916, '稗': 1917, '交': 1918, '鲨': 1919, '群': 1920, '烤': 1921, '敲': 1922, '萋': 1923, '胃': 1924, '恍': 1925, '瞄': 1926, '禧': 1927, '评': 1928, '笑': 1929, '夜': 1930, '沾': 1931, '苓': 1932, '链': 1933, '揭': 1934, '骺': 1935, '枚': 1936, 'S': 1937, '熔': 1938, '飒': 1939, '玳': 1940, '僻': 1941, '粒': 1942, '⊙': 1943, '姥': 1944, '想': 1945, '溪': 1946, '痉': 1947, '恭': 1948, '尤': 1949, '胛': 1950, '榻': 1951, '渎': 1952, '妖': 1953, '灶': 1954, '辙': 1955, '贪': 1956, '男': 1957, '卤': 1958, '固': 1959, '掺': 1960, '苋': 1961, '式': 1962, '励': 1963, '役': 1964, '沮': 1965, '上': 1966, '板': 1967, '售': 1968, '育': 1969, '鲽': 1970, '考': 1971, '疤': 1972, '铅': 1973, '情': 1974, '宼': 1975, '滤': 1976, '桐': 1977, '蛤': 1978, '狭': 1979, '蹓': 1980, '飙': 1981, '腕': 1982, '揣': 1983, '述': 1984, '智': 1985, '慎': 1986, '汪': 1987, '仗': 1988, '倾': 1989, '茍': 1990, '炎': 1991, '哆': 1992, '蜥': 1993, '蚂': 1994, '蕴': 1995, '食': 1996, 'f': 1997, '老': 1998, '抚': 1999, '穹': 2000, '订': 2001, '该': 2002, '】': 2003, '竺': 2004, '办': 2005, '悄': 2006, '肛': 2007, '砝': 2008, '病': 2009, '洽': 2010, '究': 2011, '十': 2012, '榧': 2013, '玻': 2014, '薰': 2015, '劲': 2016, '缕': 2017, '婺': 2018, '兀': 2019, '宇': 2020, '怆': 2021, '汀': 2022, '钝': 2023, '籐': 2024, '忪': 2025, '造': 2026, '宁': 2027, '熟': 2028, '缜': 2029, '］': 2030, '②': 2031, '苗': 2032, '厅': 2033, '扮': 2034, '薛': 2035, '籀': 2036, '萧': 2037, '危': 2038, '寡': 2039, '唔': 2040, '大': 2041, '悉': 2042, '准': 2043, '鳕': 2044, '梏': 2045, '哉': 2046, '盒': 2047, '霞': 2048, 'e': 2049, '懵': 2050, '激': 2051, '刊': 2052, '坚': 2053, '号': 2054, '池': 2055, '问': 2056, '碴': 2057, '实': 2058, '喜': 2059, '峥': 2060, '遁': 2061, '衬': 2062, '令': 2063, '凑': 2064, '咙': 2065, '驿': 2066, '亖': 2067, '浓': 2068, '袈': 2069, '短': 2070, '蠢': 2071, '坠': 2072, '烯': 2073, '孑': 2074, '术': 2075, '钩': 2076, '谟': 2077, '锚': 2078, '痍': 2079, '火': 2080, '稼': 2081, '毙': 2082, '慷': 2083, '辛': 2084, '土': 2085, '厦': 2086, '禽': 2087, '汁': 2088, '际': 2089, '嗬': 2090, '助': 2091, '磺': 2092, 'l': 2093, '腰': 2094, '粕': 2095, '榖': 2096, '篡': 2097, '信': 2098, '红': 2099, '1': 2100, '胡': 2101, '蒜': 2102, '犁': 2103, '博': 2104, '茎': 2105, '销': 2106, '谪': 2107, '傢': 2108, '媲': 2109, '诋': 2110, '穗': 2111, '著': 2112, '顗': 2113, '‧': 2114, '玮': 2115, '姓': 2116, '村': 2117, '锹': 2118, '打': 2119, '搜': 2120, '付': 2121, '睨': 2122, '张': 2123, '虎': 2124, '独': 2125, '图': 2126, '镌': 2127, '泮': 2128, '赚': 2129, '溼': 2130, '廿': 2131, '艇': 2132, '︰': 2133, '復': 2134, 'V': 2135, '舒': 2136, '脐': 2137, '镐': 2138, '瓣': 2139, '么': 2140, '艺': 2141, '孤': 2142, '焖': 2143, '睪': 2144, '奖': 2145, '挹': 2146, '呢': 2147, '语': 2148, '哌': 2149, '峋': 2150, '菀': 2151, '莓': 2152, '咔': 2153, '颜': 2154, '汶': 2155, '霆': 2156, '奄': 2157, '牛': 2158, '除': 2159, 'β': 2160, '蛋': 2161, '邦': 2162, '睇': 2163, '象': 2164, '冥': 2165, '蠹': 2166, '垠': 2167, '铝': 2168, '狐': 2169, '捞': 2170, '弛': 2171, '囧': 2172, '俸': 2173, '镉': 2174, '填': 2175, '娱': 2176, '舛': 2177, '矢': 2178, '猡': 2179, '媛': 2180, '帜': 2181, '垣': 2182, '谔': 2183, '唐': 2184, '宰': 2185, '鸢': 2186, '荡': 2187, '皑': 2188, '牒': 2189, '氖': 2190, '妾': 2191, '卦': 2192, '次': 2193, '击': 2194, '选': 2195, '怡': 2196, '嘛': 2197, '谒': 2198, '唑': 2199, '蘑': 2200, '浮': 2201, '籁': 2202, '恩': 2203, '锅': 2204, '纯': 2205, '牯': 2206, '瓤': 2207, '楠': 2208, '煽': 2209, '双': 2210, '恨': 2211, '祁': 2212, '脾': 2213, '疟': 2214, '阜': 2215, '豌': 2216, '癿': 2217, '遢': 2218, '与': 2219, '牀': 2220, '?': 2221, '腋': 2222, '衲': 2223, '蛙': 2224, '筝': 2225, '欺': 2226, '狄': 2227, 'B': 2228, '鹿': 2229, '庇': 2230, '绉': 2231, '筦': 2232, '卑': 2233, '沥': 2234, '叟': 2235, '霖': 2236, '萍': 2237, '瘿': 2238, '箪': 2239, '数': 2240, '停': 2241, '仿': 2242, '蝮': 2243, '扔': 2244, '府': 2245, '闪': 2246, '箫': 2247, '盈': 2248, '篾': 2249, '蛾': 2250, '浐': 2251, '汴': 2252, '桁': 2253, '锆': 2254, '皿': 2255, '尊': 2256, '揶': 2257, '糊': 2258, '蹼': 2259, '是': 2260, '乞': 2261, '臣': 2262, '礼': 2263, '？': 2264, '凄': 2265, '珀': 2266, '耘': 2267, '耽': 2268, '泻': 2269, '扪': 2270, '糜': 2271, '肼': 2272, '翟': 2273, '甥': 2274, 'D': 2275, '氚': 2276, 'M': 2277, '憋': 2278, '哭': 2279, '曦': 2280, '曳': 2281, '崎': 2282, '哚': 2283, '聚': 2284, '捉': 2285, '方': 2286, '嗨': 2287, '听': 2288, '鹮': 2289, '镧': 2290, '塌': 2291, '诣': 2292, '苟': 2293, '偌': 2294, '拇': 2295, '呛': 2296, '袤': 2297, '诬': 2298, '弩': 2299, '钡': 2300, '數': 2301, '陛': 2302, '谬': 2303, '体': 2304, '这': 2305, '莉': 2306, '祥': 2307, '雾': 2308, '晚': 2309, '哮': 2310, '耧': 2311, '日': 2312, 'g': 2313, '蓉': 2314, '蓬': 2315, '捷': 2316, '杖': 2317, '如': 2318, '徉': 2319, '席': 2320, '™': 2321, '勐': 2322, '舅': 2323, '缸': 2324, '筏': 2325, '擞': 2326, '黛': 2327, '响': 2328, '徨': 2329, '鲢': 2330, '恪': 2331, '怅': 2332, '壹': 2333, '厂': 2334, '枫': 2335, '默': 2336, '便': 2337, 'G': 2338, '饬': 2339, '嗅': 2340, '旺': 2341, '已': 2342, '萼': 2343, 'j': 2344, '癣': 2345, '伦': 2346, '览': 2347, '齖': 2348, '撕': 2349, '铁': 2350, '忸': 2351, '遑': 2352, '晏': 2353, '③': 2354, '诞': 2355, '赢': 2356, '悔': 2357, '柚': 2358, '惋': 2359, '咫': 2360, '煦': 2361, '桉': 2362, '雍': 2363, '裹': 2364, '鹘': 2365, '迴': 2366, '鲎': 2367, '缺': 2368, '趾': 2369, '勘': 2370, '童': 2371, '尅': 2372, '戎': 2373, '斛': 2374, '慧': 2375, '岌': 2376, '蜱': 2377, '文': 2378, '剋': 2379, '蓿': 2380, '档': 2381, '兖': 2382, '斧': 2383, '概': 2384, '晃': 2385, '谀': 2386, '亚': 2387, '悦': 2388, '颐': 2389, '症': 2390, '骁': 2391, '彰': 2392, '敞': 2393, '洪': 2394, '吾': 2395, '祂': 2396, '讯': 2397, '堡': 2398, '宿': 2399, '烦': 2400, '橛': 2401, '甫': 2402, '游': 2403, '讲': 2404, '捧': 2405, '藻': 2406, '蒂': 2407, '畔': 2408, '栾': 2409, '汤': 2410, '串': 2411, '岛': 2412, '朴': 2413, '挨': 2414, '砒': 2415, '逝': 2416, '桅': 2417, '旳': 2418, '阔': 2419, '他': 2420, '梗': 2421, '昕': 2422, '匝': 2423, '蘖': 2424, '利': 2425, '晦': 2426, '糬': 2427, '韡': 2428, '揄': 2429, '娶': 2430, '城': 2431, '缰': 2432, 'c': 2433, '饕': 2434, '麾': 2435, '涟': 2436, '作': 2437, '笛': 2438, '娃': 2439, '锉': 2440, '疙': 2441, '嚣': 2442, 'y': 2443, '孔': 2444, '浆': 2445, '翌': 2446, '怼': 2447, '惺': 2448, '伯': 2449, '伎': 2450, '深': 2451, '渍': 2452, '桔': 2453, '首': 2454, '冷': 2455, '缅': 2456, '玩': 2457, '个': 2458, '娼': 2459, '眷': 2460, '宠': 2461, '诽': 2462, '厉': 2463, '澡': 2464, '濛': 2465, '曩': 2466, '橙': 2467, '匿': 2468, '擅': 2469, '泛': 2470, '布': 2471, '黎': 2472, '浒': 2473, '枥': 2474, '炫': 2475, '以': 2476, '桜': 2477, '屣': 2478, '垂': 2479, '肋': 2480, '巡': 2481, '缱': 2482, '枪': 2483, '囔': 2484, '栖': 2485, '泡': 2486, '鸸': 2487, '岩': 2488, '＂': 2489, '邱': 2490, '瞬': 2491, '嬗': 2492, '露': 2493, '垛': 2494, '攷': 2495, '沱': 2496, '门': 2497, '汹': 2498, '盲': 2499, '乡': 2500, '泽': 2501, '年': 2502, '特': 2503, '为': 2504, '桎': 2505, '治': 2506, '下': 2507, '蠖': 2508, '猿': 2509, '各': 2510, '侠': 2511, '焗': 2512, '卓': 2513, '凤': 2514, '跚': 2515, '店': 2516, '仄': 2517, '带': 2518, '鞑': 2519, '蹙': 2520, '社': 2521, '刮': 2522, '悼': 2523, '些': 2524, '濯': 2525, '喝': 2526, '龚': 2527, '齣': 2528, '腔': 2529, '易': 2530, '弱': 2531, '鸹': 2532, '畀': 2533, '逸': 2534, '曜': 2535, '效': 2536, '艘': 2537, '钐': 2538, '陀': 2539, '主': 2540, '呕': 2541, '嘘': 2542, '肢': 2543, '雄': 2544, '伱': 2545, '瀛': 2546, '球': 2547, '捶': 2548, '颊': 2549, '娄': 2550, '蒸': 2551, '甸': 2552, '幻': 2553, '鬓': 2554, '屁': 2555, '渠': 2556, '绵': 2557, '湮': 2558, '盎': 2559, '篆': 2560, '讶': 2561, '砻': 2562, '民': 2563, '斥': 2564, '丌': 2565, '钊': 2566, '义': 2567, '劳': 2568, '苕': 2569, '卫': 2570, '吝': 2571, '戳': 2572, '噼': 2573, '侥': 2574, '枘': 2575, '瑰': 2576, '刽': 2577, '谨': 2578, '忡': 2579, '耿': 2580, '含': 2581, '甚': 2582, '·': 2583, '黠': 2584, '阂': 2585, '盏': 2586, '妈': 2587, '峡': 2588, '隈': 2589, '姿': 2590, '糅': 2591, '钪': 2592, '箕': 2593, '谴': 2594, '岐': 2595, '远': 2596, '蒿': 2597, '砌': 2598, '巨': 2599, '7': 2600, '角': 2601, '镬': 2602, '寿': 2603, '傀': 2604, '惨': 2605, '焊': 2606, '俄': 2607, '妙': 2608, '萦': 2609, '草': 2610, '赭': 2611, '眬': 2612, '四': 2613, '昆': 2614, '狱': 2615, '机': 2616, '约': 2617, '忆': 2618, '验': 2619, '惹': 2620, '椭': 2621, '师': 2622, '侍': 2623, '咂': 2624, '蟾': 2625, '咏': 2626, '吭': 2627, '铎': 2628, '符': 2629, '莲': 2630, '献': 2631, '殒': 2632, '罕': 2633, '齿': 2634, '钵': 2635, '彝': 2636, '伏': 2637, '酞': 2638, '(': 2639, '爸': 2640, '邓': 2641, '饶': 2642, '夕': 2643, '‰': 2644, '颂': 2645, '滞': 2646, '钜': 2647, '总': 2648, '累': 2649, '锐': 2650, '埭': 2651, '俏': 2652, '蒡': 2653, '咀': 2654, '促': 2655, '干': 2656, '溜': 2657, '犹': 2658, '卢': 2659, '诅': 2660, '茕': 2661, '黏': 2662, '代': 2663, '需': 2664, '矫': 2665, '冻': 2666, '添': 2667, '姚': 2668, '告': 2669, '遵': 2670, '享': 2671, '羌': 2672, '冉': 2673, '馄': 2674, '硕': 2675, '京': 2676, '孵': 2677, '醉': 2678, '尖': 2679, '宸': 2680, '奎': 2681, '设': 2682, '晶': 2683, '璐': 2684, '辊': 2685, '逞': 2686, '掉': 2687, '恃': 2688, '萱': 2689, '羟': 2690, '瑶': 2691, 'k': 2692, '㱀': 2693, '郗': 2694, '窃': 2695, '列': 2696, '柏': 2697, '市': 2698, '美': 2699, '$': 2700, '’': 2701, '麻': 2702, '膛': 2703, '溅': 2704, '袜': 2705, '\\n': 2706, '割': 2707, '绅': 2708, '肓': 2709, '冒': 2710, '倒': 2711, '侏': 2712, '漫': 2713, '亭': 2714, '锤': 2715, '铛': 2716, '线': 2717, '暖': 2718, '钱': 2719, '琼': 2720, '矛': 2721, '惊': 2722, '鼬': 2723, '醴': 2724, '氪': 2725, '公': 2726, '痰': 2727, '竭': 2728, '䐜': 2729, '裡': 2730, '磊': 2731, '阮': 2732, '乳': 2733, '猥': 2734, '片': 2735, '剑': 2736, '踯': 2737, '壳': 2738, '氰': 2739, '骤': 2740, '旖': 2741, '寞': 2742, '篦': 2743, '围': 2744, '搅': 2745, '籍': 2746, '鲍': 2747, '驶': 2748, '悲': 2749, '翭': 2750, '八': 2751, '璞': 2752, '：': 2753, '函': 2754, 'o': 2755, '毽': 2756, '昧': 2757, '瘘': 2758, '咚': 2759, '称': 2760, '胸': 2761, '颈': 2762, '二': 2763, '烨': 2764, '酯': 2765, '啮': 2766, '珍': 2767, '罏': 2768, '底': 2769, '窖': 2770, '修': 2771, '鹚': 2772, '篇': 2773, '履': 2774, '鸬': 2775, '让': 2776, '骐': 2777, '骝': 2778, '常': 2779, '明': 2780, '胖': 2781, '孚': 2782, '蚁': 2783, '、': 2784, '鸭': 2785, '的': 2786, '躇': 2787, '奸': 2788, '翱': 2789, '嘧': 2790, '先': 2791, '•': 2792, '晾': 2793, '锣': 2794, '系': 2795, '孽': 2796, '议': 2797, '惘': 2798, '拯': 2799, '翠': 2800, '邯': 2801, '偿': 2802, '炮': 2803, '妇': 2804, '骆': 2805, '凰': 2806, '千': 2807, '匙': 2808, '！': 2809, '叶': 2810, '圳': 2811, '掼': 2812, '曼': 2813, '隐': 2814, '睐': 2815, '黯': 2816, '谁': 2817, '骨': 2818, '腮': 2819, '邳': 2820, '计': 2821, '搔': 2822, '杆': 2823, '蓄': 2824, '菌': 2825, '现': 2826, '沫': 2827, '扳': 2828, '\\u2005': 2829, '服': 2830, '戒': 2831, '螯': 2832, '侧': 2833, '琴': 2834, '趸': 2835, '衫': 2836, '窄': 2837, '裟': 2838, '芝': 2839, '岚': 2840, '退': 2841, '茅': 2842, '蛸': 2843, '喀': 2844, '囤': 2845, '芩': 2846, '牲': 2847, '逮': 2848, '矿': 2849, '波': 2850, '满': 2851, '衰': 2852, '邋': 2853, '潜': 2854, '皂': 2855, '缬': 2856, '持': 2857, '升': 2858, '州': 2859, '\\t': 2860, '警': 2861, '绘': 2862, '踞': 2863, '\\xad': 2864, '泰': 2865, '郑': 2866, '睑': 2867, '熄': 2868, '钒': 2869, '补': 2870, '歉': 2871, '蜿': 2872, '傈': 2873, '螽': 2874, '绚': 2875, '巢': 2876, '回': 2877, '隙': 2878, '睁': 2879, '发': 2880, '彤': 2881, '哒': 2882, '孳': 2883, '拔': 2884, '键': 2885, '瘙': 2886, '庙': 2887, '憬': 2888, '汐': 2889, '哦': 2890, '圾': 2891, '铒': 2892, '域': 2893, '扼': 2894, '霎': 2895, '潢': 2896, '鸪': 2897, '齐': 2898, '拮': 2899, '幌': 2900, '丶': 2901, '陨': 2902, '肖': 2903, '坡': 2904, '央': 2905, '鳗': 2906, '蚕': 2907, '峭': 2908, '取': 2909, '陆': 2910, '屯': 2911, '港': 2912, '研': 2913, '蒲': 2914, '郝': 2915, '肉': 2916, '襦': 2917, '葩': 2918, '渣': 2919, '丹': 2920, '識': 2921, '样': 2922, 's': 2923, '#': 2924, 'L': 2925, '醌': 2926, '砖': 2927, '技': 2928, '旅': 2929, '苍': 2930, '龟': 2931, '呱': 2932, '人': 2933, '众': 2934, '气': 2935, '极': 2936, '盾': 2937, '折': 2938, '恰': 2939, '武': 2940, '关': 2941, '用': 2942, '威': 2943, '性': 2944, '噉': 2945, '=': 2946, '若': 2947, '踌': 2948, '指': 2949, '花': 2950, '峙': 2951, '综': 2952, '入': 2953, '湾': 2954, '翃': 2955, '飚': 2956, '叭': 2957, '扒': 2958, '孙': 2959, '韶': 2960, '稣': 2961, '讬': 2962, '妓': 2963, '敢': 2964, '匈': 2965, '肴': 2966, '嗽': 2967, '曝': 2968, '狗': 2969, '东': 2970, '祗': 2971, '载': 2972, '熵': 2973, '惮': 2974, '浏': 2975, '鼓': 2976, '金': 2977, '芽': 2978, '晌': 2979, '严': 2980, ';': 2981, '戊': 2982, '慑': 2983, '瓶': 2984, '叩': 2985, '雁': 2986, '婊': 2987, '嗲': 2988, '爹': 2989, '践': 2990, '普': 2991, '按': 2992, '煞': 2993, '柬': 2994, '甾': 2995, '麝': 2996, 'ｌ': 2997, '龈': 2998, '榭': 2999, '扦': 3000, '愚': 3001, '缑': 3002, '把': 3003, '醪': 3004, '捄': 3005, '佳': 3006, '必': 3007, '冲': 3008, '肃': 3009, '埃': 3010, '页': 3011, '茜': 3012, '乐': 3013, '几': 3014, '慌': 3015, '瑙': 3016, '财': 3017, '蝠': 3018, '龙': 3019, '滨': 3020, '峰': 3021, '可': 3022, '端': 3023, '懈': 3024, '湎': 3025, '瞰': 3026, '涔': 3027, '璃': 3028, '仍': 3029, '豋': 3030, '嘴': 3031, '颧': 3032, '胨': 3033, '呜': 3034, '面': 3035, '俱': 3036, '止': 3037, '吕': 3038, '锱': 3039, '香': 3040, '手': 3041, '亏': 3042, '镰': 3043, '瘀': 3044, '凡': 3045, '偻': 3046, '悸': 3047, '鞠': 3048, '肚': 3049, '索': 3050, '渊': 3051, '⑥': 3052, '正': 3053, '掰': 3054, '缛': 3055, '翁': 3056, '岱': 3057, '弹': 3058, '车': 3059, '澄': 3060, '珐': 3061, '﹑': 3062, '虾': 3063, '藉': 3064, '猝': 3065, '搭': 3066, '痐': 3067, '谘': 3068, '幽': 3069, '韵': 3070, '颓': 3071, '撬': 3072, '贱': 3073, '蕨': 3074, '颗': 3075, '谏': 3076, 'γ': 3077, '牦': 3078, '藩': 3079, '营': 3080, '皱': 3081, '承': 3082, '矜': 3083, '舔': 3084, '呈': 3085, '溢': 3086, '黜': 3087, '由': 3088, '源': 3089, '返': 3090, '塑': 3091, '务': 3092, '肱': 3093, '够': 3094, '赡': 3095, '忧': 3096, '瓴': 3097, '龉': 3098, '九': 3099, '渰': 3100, '逅': 3101, '疲': 3102, '3': 3103, '槁': 3104, '霍': 3105, '阱': 3106, '搐': 3107, '唁': 3108, '茂': 3109, '鱿': 3110, '餐': 3111, '佼': 3112, '药': 3113, '萌': 3114, '旨': 3115, '芒': 3116, '砣': 3117, '孪': 3118, '倪': 3119, '／': 3120, '骚': 3121, '肺': 3122, '瘟': 3123, '水': 3124, '散': 3125, '谶': 3126, '伙': 3127, '荪': 3128, '夥': 3129, '羔': 3130, '寨': 3131, '跟': 3132, '掌': 3133, '瞪': 3134, '篝': 3135, '蜊': 3136, '妥': 3137, '犬': 3138, '希': 3139, '阀': 3140, '厮': 3141, '咯': 3142, '踹': 3143, '琵': 3144, '阶': 3145, '赊': 3146, '２': 3147, '馥': 3148, '莞': 3149, '鲸': 3150, '巽': 3151, '殷': 3152, '漂': 3153, '触': 3154, '阖': 3155, '阊': 3156, '遍': 3157, '导': 3158, '糖': 3159, '厌': 3160, '霉': 3161, '江': 3162, '胥': 3163, '绶': 3164, '褴': 3165, '疣': 3166, '比': 3167, '沵': 3168, '罟': 3169, '向': 3170, '─': 3171, '瘩': 3172, '诫': 3173, '牢': 3174, '撒': 3175, '喉': 3176, '私': 3177, '习': 3178, '臃': 3179, '农': 3180, '笼': 3181, '拜': 3182, '待': 3183, '滇': 3184, '迟': 3185, '匣': 3186, '閙': 3187, '苹': 3188, '斩': 3189, '腹': 3190, '吟': 3191, '髋': 3192, '杵': 3193, '拣': 3194, '友': 3195, '感': 3196, '溧': 3197, '澳': 3198, '骋': 3199, '忿': 3200, '归': 3201, '俭': 3202, '绿': 3203, '防': 3204, '…': 3205, '叽': 3206, '苏': 3207, '就': 3208, '矮': 3209, '貇': 3210, '椅': 3211, '珰': 3212, '栀': 3213, '铭': 3214, \"'\": 3215, '畸': 3216, '菖': 3217, '4': 3218, '和': 3219, '祷': 3220, '乎': 3221, '判': 3222, '妞': 3223, '识': 3224, '埕': 3225, '慕': 3226, '劾': 3227, '猛': 3228, '兜': 3229, '珈': 3230, '态': 3231, '虐': 3232, '禅': 3233, '蚓': 3234, '框': 3235, '歼': 3236, '规': 3237, '衍': 3238, '。': 3239, '王': 3240, '膻': 3241, '疝': 3242, '吹': 3243, '吃': 3244, '亵': 3245, '渺': 3246, '逎': 3247, '匡': 3248, '薄': 3249, '魅': 3250, '瀚': 3251, '痨': 3252, '纬': 3253, '证': 3254, '兼': 3255, '辟': 3256, '圆': 3257, '午': 3258, '像': 3259, '醯': 3260, '猬': 3261, '膘': 3262, '蔗': 3263, '去': 3264, '躺': 3265, '痢': 3266, '活': 3267, '湍': 3268, '琢': 3269, '漉': 3270, '派': 3271, '镇': 3272, 'Z': 3273, '贴': 3274, '嗥': 3275, '儿': 3276, 'T': 3277, '亡': 3278, '枸': 3279, '讪': 3280, '裁': 3281, '迭': 3282, '舆': 3283, '泥': 3284, '仲': 3285, '禊': 3286, '芥': 3287, '椰': 3288, '步': 3289, '囱': 3290, '站': 3291, '闳': 3292, '一': 3293, '♫': 3294, '砷': 3295, '滴': 3296, '雯': 3297, '祐': 3298, '家': 3299, '岭': 3300, '珊': 3301, '脲': 3302, '蜉': 3303, '坟': 3304, '叮': 3305, '赈': 3306, '哎': 3307, '鹩': 3308, '辄': 3309, '偕': 3310, '棱': 3311, '转': 3312, '槟': 3313, '汲': 3314, '昴': 3315, '戈': 3316, '霄': 3317, '欧': 3318, '兄': 3319, '琪': 3320, '涚': 3321, '撙': 3322, '炽': 3323, '瓯': 3324, '票': 3325, '穷': 3326, '缟': 3327, '榄': 3328, '丫': 3329, '娜': 3330, '速': 3331, 'm': 3332, '罩': 3333, '吮': 3334, '垦': 3335, '崩': 3336, '库': 3337, '@': 3338, '焱': 3339, '何': 3340, 'J': 3341, '艳': 3342, '覆': 3343, '匍': 3344, '芜': 3345, '巯': 3346, '道': 3347, '啕': 3348, '踱': 3349, '鹑': 3350, '柩': 3351, '真': 3352, '演': 3353, '价': 3354, '槐': 3355, '寐': 3356, '蓝': 3357, '瘠': 3358, '倍': 3359, '鹜': 3360, '扁': 3361, '巍': 3362, '限': 3363, '焓': 3364, '浩': 3365, '苞': 3366, '瓢': 3367, '喨': 3368, '伴': 3369, '强': 3370, '欸': 3371, '度': 3372, 'é': 3373, '炭': 3374, '殖': 3375, '伶': 3376, '缓': 3377, '医': 3378, '瘴': 3379, '绍': 3380, '梭': 3381, '森': 3382, '煌': 3383, '嵗': 3384, '丢': 3385, '痫': 3386, '焯': 3387, '悠': 3388, '县': 3389, '南': 3390, '瘫': 3391, '梦': 3392, '髁': 3393, '魁': 3394, '苎': 3395, '茛': 3396, '辅': 3397, '度': 3398, '碎': 3399, '浜': 3400, '行': 3401, '劈': 3402, '［': 3403, '罹': 3404, '急': 3405, '痊': 3406, '胆': 3407, '翥': 3408, '郊': 3409, '蛹': 3410, '吸': 3411, '−': 3412, '伸': 3413, '筛': 3414, '幂': 3415, '有': 3416, '捩': 3417, '崽': 3418, '恋': 3419, '坷': 3420, '磐': 3421, '静': 3422, '磁': 3423, '魆': 3424, '件': 3425, '氩': 3426, '迦': 3427, '驾': 3428, '懦': 3429, '显': 3430, '做': 3431, '蝉': 3432, 'v': 3433, 'Ω': 3434, '苄': 3435, '沙': 3436, '氓': 3437, '疏': 3438, '桥': 3439, '揩': 3440, '又': 3441, '骶': 3442, '宽': 3443, '衾': 3444, '执': 3445, '救': 3446, '箱': 3447, '烷': 3448, '免': 3449, '拘': 3450, '±': 3451, '创': 3452, '苷': 3453, '婉': 3454, '虚': 3455, '蜕': 3456, '萎': 3457, '硼': 3458, '斌': 3459, '蔫': 3460, '醺': 3461, '婶': 3462, '挚': 3463, 'ö': 3464, '蝙': 3465, '允': 3466, '抹': 3467, '驰': 3468, '怒': 3469, '趁': 3470, '逵': 3471, '羡': 3472, '荣': 3473, '乒': 3474, '钧': 3475, '热': 3476, '宾': 3477, '»': 3478, '纳': 3479, '费': 3480, 'x': 3481, '鸻': 3482, '葆': 3483, '监': 3484, '莅': 3485, '裕': 3486, '裂': 3487, '挺': 3488, '神': 3489, '膝': 3490, '蕾': 3491, '誓': 3492, '谧': 3493, '捱': 3494, '躬': 3495, '聆': 3496, '供': 3497, 'á': 3498, '华': 3499, '括': 3500, '歇': 3501, 'ü': 3502, '豪': 3503, '痔': 3504, '咁': 3505, '喵': 3506, '徬': 3507, '浪': 3508, '诧': 3509, '背': 3510, '踏': 3511, '佝': 3512, '羞': 3513, '喽': 3514, '详': 3515, '燥': 3516, '杠': 3517, '股': 3518, '，': 3519, '竿': 3520, '蝌': 3521, '琳': 3522, '湖': 3523, '寰': 3524, '驻': 3525, '召': 3526, '茔': 3527, '趴': 3528, '墅': 3529, '饵': 3530, '窕': 3531, '平': 3532, '谷': 3533, '年': 3534, '陪': 3535, '彬': 3536, '骜': 3537, '暧': 3538, '绦': 3539, '偏': 3540, '麓': 3541, '茱': 3542, '掀': 3543, '内': 3544, '芹': 3545, '鞍': 3546, '饰': 3547, '壤': 3548, '伫': 3549, '\\u200b': 3550, '笤': 3551, '悖': 3552, '裢': 3553, '之': 3554, '附': 3555, '雏': 3556, '辱': 3557, '迹': 3558, '顿': 3559, '磙': 3560, '惇': 3561, '赝': 3562, '浬': 3563, '太': 3564, '娇': 3565, '房': 3566, '丘': 3567, '鸵': 3568, '撩': 3569, '呋': 3570, '〉': 3571, '馨': 3572, '沁': 3573, '剥': 3574, '往': 3575, '赐': 3576, '卿': 3577, '鏊': 3578, '列': 3579, '辘': 3580, '弑': 3581, '绺': 3582, '悌': 3583, '凌': 3584, '跨': 3585, '勿': 3586, '贵': 3587, '撷': 3588, '芷': 3589, '吧': 3590, '骏': 3591, '笆': 3592, '惰': 3593, '倦': 3594, '鼾': 3595, '墉': 3596, '物': 3597, '硝': 3598, '籼': 3599, '袒': 3600, '酶': 3601, '惬': 3602, '赂': 3603, '钤': 3604, '浇': 3605, '脖': 3606, '潟': 3607, '状': 3608, '怦': 3609, '碉': 3610, '搞': 3611, '逻': 3612, 'E': 3613, '脯': 3614, '睚': 3615, '亢': 3616, '留': 3617, '河': 3618, '姊': 3619, '轧': 3620, '叨': 3621, '拆': 3622, '奢': 3623, '苔': 3624, '七': 3625, '漓': 3626, '蟒': 3627, '镯': 3628, '翘': 3629, '燃': 3630, '孜': 3631, '闽': 3632, '恒': 3633, '炼': 3634, '臻': 3635, '酥': 3636, '对': 3637, '屏': 3638, '鹭': 3639, '耐': 3640, '闲': 3641, '稠': 3642, '匩': 3643, '虱': 3644, ':': 3645, '郡': 3646, '械': 3647, '莫': 3648, '砜': 3649, '料': 3650, '苑': 3651, '配': 3652, '剿': 3653, '紫': 3654, '柱': 3655, '近': 3656, '哧': 3657, 'q': 3658, '粽': 3659, '氛': 3660, '钨': 3661, '氟': 3662, '诵': 3663, '而': 3664, '淡': 3665, '癸': 3666, '堕': 3667, '脓': 3668, '到': 3669, '障': 3670, '眸': 3671, '反': 3672, '酝': 3673, '剔': 3674, '寸': 3675, '轩': 3676, '跃': 3677, '吁': 3678, '荐': 3679, '旱': 3680, '靖': 3681, '蝽': 3682, '刺': 3683, '彗': 3684, 'U': 3685, '谅': 3686, '幅': 3687, '引': 3688, '诡': 3689, '镑': 3690, '饱': 3691, '僵': 3692, '百': 3693, '孕': 3694, '疸': 3695, '溥': 3696, '剖': 3697, '攀': 3698, '蠕': 3699, '摈': 3700, '昼': 3701, '谩': 3702, '袍': 3703, '；': 3704, '缭': 3705, '摆': 3706, '恽': 3707, '妳': 3708, '锋': 3709, '蝎': 3710, '炙': 3711, '灾': 3712, '勤': 3713, '沸': 3714, '出': 3715, '贼': 3716, '怪': 3717, '骼': 3718, '莳': 3719, '街': 3720, '瘤': 3721, '膺': 3722, '匐': 3723, '龛': 3724, '乾': 3725, '嘬': 3726, '唯': 3727, '铆': 3728, '淹': 3729, '酗': 3730, '腐': 3731, '抑': 3732, '矩': 3733, '聊': 3734, '狂': 3735, '繁': 3736, '绷': 3737, '缡': 3738, '筑': 3739, '税': 3740, '坏': 3741, '槃': 3742, '铂': 3743, '觊': 3744, '驭': 3745, '晰': 3746, '缚': 3747, '翔': 3748, '蜚': 3749, '茁': 3750, '岁': 3751, '领': 3752, '媒': 3753, '容': 3754, '拟': 3755, '＃': 3756, '翫': 3757, '尉': 3758, '酊': 3759, '畜': 3760, '垫': 3761, '嘌': 3762, '延': 3763, '蕙': 3764, '混': 3765, '青': 3766, '钹': 3767, '案': 3768, '煤': 3769, '藤': 3770, '芐': 3771, '嫣': 3772, '鲤': 3773, '偶': 3774, '罄': 3775, '莠': 3776, '﹐': 3777, '夺': 3778, '败': 3779, '坞': 3780, '汞': 3781, '牠': 3782, '锺': 3783, '颠': 3784, '挫': 3785, '隶': 3786, '祚': 3787, '来': 3788, '硬': 3789, '挽': 3790, '眺': 3791, '钢': 3792, '润': 3793, '拒': 3794, '建': 3795, '覜': 3796, '屉': 3797, '咨': 3798, '脷': 3799, '札': 3800, '菇': 3801, '欲': 3802, '啶': 3803, '桓': 3804, '桦': 3805, '巬': 3806, '枳': 3807, '诛': 3808, '您': 3809, '楣': 3810, '礴': 3811, '啦': 3812, '恬': 3813, '涯': 3814, '唾': 3815, '俨': 3816, '袱': 3817, '饥': 3818, '缦': 3819, '氯': 3820, '缫': 3821, '腈': 3822, '鉆': 3823, '尿': 3824, '跆': 3825, '撼': 3826, '醮': 3827, '诌': 3828, '懊': 3829, '–': 3830, '冈': 3831, '液': 3832, '吿': 3833, '碧': 3834, '诺': 3835, '加': 3836, '姝': 3837, '宕': 3838, '逍': 3839, '帷': 3840, '怨': 3841, '埗': 3842, '淆': 3843, '裳': 3844, '匕': 3845, '朊': 3846, '褐': 3847, '缂': 3848, '颀': 3849, '俪': 3850, '酿': 3851, '中': 3852, '氧': 3853, '右': 3854, '假': 3855, '掘': 3856, '铺': 3857, 'ｓ': 3858, '胼': 3859, '芫': 3860, '换': 3861, '铯': 3862, 'A': 3863, '-': 3864, '俗': 3865, '牡': 3866, '仃': 3867, '逊': 3868, '踮': 3869, '枇': 3870, '疱': 3871, '翦': 3872, '韬': 3873, '噻': 3874, '旌': 3875, '诀': 3876, '胰': 3877, '咪': 3878, '鼻': 3879, '净': 3880, '铼': 3881, '骄': 3882, '凶': 3883, 'π': 3884, '浑': 3885, '将': 3886, '枯': 3887, '悴': 3888, '鳐': 3889, '谰': 3890, '攥': 3891, '忽': 3892, '蜈': 3893, '因': 3894, '霓': 3895, '▪': 3896, '候': 3897, '晤': 3898, '瀑': 3899, '蔡': 3900, '嚥': 3901, '黔': 3902, '杰': 3903, '葶': 3904, '秘': 3905, '耵': 3906, '拾': 3907, '支': 3908, '获': 3909, '虔': 3910, '\"': 3911, '葵': 3912, '怂': 3913, '增': 3914, '葬': 3915, '嫂': 3916, '壁': 3917, '挎': 3918, '腍': 3919, '徳': 3920, '廷': 3921, '侄': 3922, '岔': 3923, '碲': 3924, '观': 3925, '尹': 3926, '揽': 3927, '略': 3928, '腆': 3929, '滹': 3930, '透': 3931, '杨': 3932, '亟': 3933, '瞧': 3934, '‖': 3935, '忠': 3936, '储': 3937, '矶': 3938, '较': 3939, '淳': 3940, '尾': 3941, '策': 3942, '萝': 3943, '蓖': 3944, '暸': 3945, '捲': 3946, '卧': 3947, '倡': 3948, '渝': 3949, '罗': 3950, '攒': 3951, '刚': 3952, '嵌': 3953, '怏': 3954, '例': 3955, '辖': 3956, '寮': 3957, '萄': 3958, '怔': 3959, '沦': 3960, '舞': 3961, '什': 3962, '英': 3963, '进': 3964, '颍': 3965, '层': 3966, '嵘': 3967, 'è': 3968, '钳': 3969, '猾': 3970, '菸': 3971, '踊': 3972, '灼': 3973, '鹬': 3974, '娥': 3975, '组': 3976, '足': 3977, '老': 3978, '卡': 3979, '茵': 3980, '卸': 3981, '瑯': 3982, '渗': 3983, '嬴': 3984, '怎': 3985, '盗': 3986, '海': 3987, '鹃': 3988, '裸': 3989, '蠊': 3990, '记': 3991, '蕊': 3992, '适': 3993, '易': 3994, '闻': 3995, '涅': 3996, '淘': 3997, '谣': 3998, '窟': 3999, '钓': 4000, '爱': 4001, '胚': 4002, '抽': 4003, '豇': 4004, '掏': 4005, '刁': 4006, '户': 4007, '莺': 4008, '丽': 4009, '馆': 4010, '滂': 4011, '尕': 4012, '丙': 4013, '拎': 4014, '序': 4015, '棘': 4016, '觑': 4017, '辨': 4018, '楤': 4019, '璧': 4020, '吖': 4021, '丈': 4022, '侬': 4023, '叹': 4024, '怜': 4025, '喇': 4026, '骷': 4027, '鷾': 4028, '僖': 4029, '厨': 4030, '疼': 4031, '抛': 4032, '鲜': 4033, '竣': 4034, '漪': 4035, '昔': 4036, '撇': 4037, '寝': 4038, '蜣': 4039, '流': 4040, '蚤': 4041, '量': 4042, '挠': 4043, '怩': 4044, '嘹': 4045, '●': 4046, '诊': 4047, '６': 4048, '疾': 4049, '操': 4050, '量': 4051, '鞅': 4052, '柔': 4053, '铄': 4054, '萤': 4055, '恣': 4056, '虞': 4057, '驼': 4058, '饽': 4059, '滕': 4060, '骛': 4061, '钟': 4062, '坎': 4063, '畏': 4064, '毕': 4065, '酉': 4066, '丰': 4067, '贤': 4068, '忘': 4069, '咣': 4070, '铬': 4071, '钽': 4072, '未': 4073, '诃': 4074, '同': 4075, '钴': 4076, '滥': 4077, '蟹': 4078, '踉': 4079, '蚶': 4080, '悬': 4081, '胶': 4082, '姗': 4083, '卒': 4084, '撅': 4085, '脒': 4086, '纹': 4087, '濒': 4088, '氢': 4089, '舵': 4090, '菜': 4091, '泱': 4092, '拖': 4093, '靛': 4094, '抄': 4095, '幡': 4096, '佻': 4097, '阙': 4098, '弭': 4099, '妩': 4100, '帆': 4101, '弈': 4102, '鞋': 4103, '启': 4104, '郭': 4105, '杂': 4106, '衞': 4107, '奘': 4108, '竖': 4109, '督': 4110, '踵': 4111, '影': 4112, '抓': 4113, '祢': 4114, '蛮': 4115, '碰': 4116, '骂': 4117, '宝': 4118, '叛': 4119, '壅': 4120, '锌': 4121, '徵': 4122, '雅': 4123, '秩': 4124, '欣': 4125, '湛': 4126, '队': 4127, '摘': 4128, '援': 4129, '拍': 4130, '棺': 4131, '饭': 4132, '失': 4133, '罘': 4134, '巧': 4135, '敌': 4136, '炳': 4137, '祝': 4138, '律': 4139, '巴': 4140, '庭': 4141, '题': 4142, '珩': 4143, '稳': 4144, '脏': 4145, '邂': 4146, '掩': 4147, '纵': 4148, '但': 4149, '襞': 4150, '帛': 4151, '砰': 4152, '颏': 4153, '箍': 4154, '庄': 4155, '庞': 4156, '冶': 4157, '殃': 4158, '皈': 4159, '棍': 4160, '弟': 4161, '梁': 4162, '渥': 4163, '崆': 4164, '脉': 4165, '碗': 4166, '墁': 4167, '昌': 4168, '蚱': 4169, '粼': 4170, '掷': 4171, '具': 4172, '迄': 4173, '瞥': 4174, '曲': 4175, '酰': 4176, '逖': 4177, '淅': 4178, '咳': 4179, '忙': 4180, '棉': 4181, '荆': 4182, '还': 4183, '啸': 4184, '诙': 4185, '均': 4186, '弥': 4187, 'Ⅲ': 4188, '玷': 4189, '蛉': 4190, '藕': 4191, '袖': 4192, '奥': 4193, '释': 4194, '搽': 4195, '纭': 4196, '胄': 4197, '咕': 4198, '妍': 4199, '徊': 4200, '赠': 4201, '扉': 4202, '醣': 4203, '酸': 4204, '菅': 4205, '赛': 4206, '艾': 4207, '秃': 4208, 'Ñ': 4209, '闗': 4210, '貘': 4211, '堿': 4212, '贡': 4213, '缴': 4214, '诏': 4215, '黄': 4216, '缔': 4217, '丸': 4218, '周': 4219, '炔': 4220, '摁': 4221, '留': 4222, '幕': 4223, '沪': 4224, '爬': 4225, '埔': 4226, '舂': 4227, '找': 4228, 'μ': 4229, '掠': 4230, '舐': 4231, '怠': 4232, '肘': 4233, '撂': 4234, '养': 4235, '茧': 4236, '帅': 4237, '烛': 4238, '摇': 4239, '认': 4240, '鄞': 4241, '璋': 4242, '薹': 4243, '遗': 4244, '叠': 4245, '谭': 4246, '秧': 4247, '负': 4248, '福': 4249, '遄': 4250, '宦': 4251, '摒': 4252, '伤': 4253, '丛': 4254, '接': 4255, '债': 4256, '鷺': 4257, '份': 4258, '唢': 4259, '藓': 4260, '奕': 4261, '曹': 4262, '趣': 4263, '蜜': 4264, '琶': 4265, 'u': 4266, '瑾': 4267, '墨': 4268, '〇': 4269, '靶': 4270, '糟': 4271, '讹': 4272, '瓿': 4273, '啡': 4274, '轶': 4275, '彪': 4276, '铢': 4277, '錄': 4278, '渐': 4279, '颅': 4280, '螨': 4281, '浴': 4282, '噢': 4283, '惯': 4284, '钌': 4285, '犷': 4286, '幔': 4287, '条': 4288, '更': 4289, '胧': 4290, '臆': 4291, '货': 4292, '昀': 4293, '万': 4294, '巫': 4295, '冬': 4296, '界': 4297, '睬': 4298, '洱': 4299, '霾': 4300, '钇': 4301, '沏': 4302, '碑': 4303, '祸': 4304, '瑜': 4305, '项': 4306, '杞': 4307, '骰': 4308, '初': 4309, '铵': 4310, '耦': 4311, 'w': 4312, '闹': 4313, '靓': 4314, '痣': 4315, '橼': 4316, '啃': 4317, '兹': 4318, '阗': 4319, '挲': 4320, '丐': 4321, '互': 4322, '傅': 4323, '责': 4324, '熠': 4325, '挛': 4326, '<': 4327, '嗪': 4328, '追': 4329, '喂': 4330, '鞭': 4331, '翰': 4332, '鲑': 4333, '鸯': 4334, '腑': 4335, '憧': 4336, '谋': 4337, '飃': 4338, '诗': 4339, '瞩': 4340, '额': 4341, '・': 4342, '蓐': 4343, '超': 4344, '副': 4345, '蘸': 4346, '窈': 4347, '弃': 4348, '晋': 4349, '篓': 4350, '铮': 4351, '巾': 4352, '恁': 4353, '别': 4354, '迂': 4355, '慵': 4356, '刃': 4357, '杷': 4358, '膜': 4359, '镓': 4360, '醋': 4361, '德': 4362, '吐': 4363, '觅': 4364, '矾': 4365, '涸': 4366, '啄': 4367, '昵': 4368, '舟': 4369, '钣': 4370, '蛊': 4371, '笃': 4372, '灌': 4373, '葫': 4374, '澎': 4375, '辽': 4376, '榷': 4377, '熙': 4378, '疹': 4379, '挥': 4380, '釜': 4381, '迪': 4382, '栒': 4383, '冼': 4384, '”': 4385, '举': 4386, '芏': 4387, '妄': 4388, '阳': 4389, '9': 4390, '①': 4391, '柑': 4392, '寓': 4393, '逗': 4394, '俘': 4395, '邹': 4396, '景': 4397, '扩': 4398, '亥': 4399, '坍': 4400, '李': 4401, '朽': 4402, '愿': 4403, '莎': 4404, '闸': 4405, '矗': 4406, '」': 4407, '倭': 4408, '胱': 4409, '绪': 4410, '勾': 4411, '浊': 4412, '蝇': 4413, '遂': 4414, '噌': 4415, '碳': 4416, '梳': 4417, '喱': 4418, '闩': 4419, '赘': 4420, '△': 4421, '鹦': 4422, '夔': 4423, '渤': 4424, '哽': 4425, '胺': 4426, '榜': 4427, '差': 4428, '牧': 4429, '朗': 4430, '佑': 4431, '攫': 4432, '怕': 4433, 'ô': 4434, 'Ⅱ': 4435, '赞': 4436, '晨': 4437, '馀': 4438, '莰': 4439, 'µ': 4440, '陡': 4441, '渔': 4442, '妒': 4443, '珙': 4444, '拢': 4445, '试': 4446, '巳': 4447, '薜': 4448, '旋': 4449, '鲫': 4450, '隧': 4451, '了': 4452, '秭': 4453, '婧': 4454, '替': 4455, '秆': 4456, '风': 4457, '沼': 4458, '针': 4459, '茶': 4460, '化': 4461, '潘': 4462, '蜒': 4463, '瑚': 4464, '权': 4465, '翻': 4466, '顽': 4467, '或': 4468, '娆': 4469, '路': 4470, '谆': 4471, '啜': 4472, '善': 4473, '褪': 4474, '釆': 4475, '橱': 4476, '鑫': 4477, '滋': 4478, '蔚': 4479, '羁': 4480, '浸': 4481, '杈': 4482, '贸': 4483, '很': 4484, '褛': 4485, '牍': 4486, '类': 4487, '膈': 4488, '%': 4489, '纺': 4490, '贯': 4491, '功': 4492, '啫': 4493, '丝': 4494, '厘': 4495, '全': 4496, '携': 4497, '稚': 4498, '骑': 4499, '受': 4500, '蜴': 4501, '铉': 4502, '羉': 4503, '盆': 4504, '捋': 4505, '湃': 4506, '牌': 4507, '乱': 4508, '劝': 4509, '沂': 4510, '茯': 4511, '澈': 4512, '亨': 4513, '蚬': 4514, '曙': 4515, '窸': 4516, '排': 4517, '哺': 4518, '蹄': 4519, '睾': 4520, '啼': 4521, '啬': 4522, '磷': 4523, '萘': 4524, '贺': 4525, '恺': 4526, '榨': 4527, '译': 4528, '不': 4529, '宅': 4530, '禀': 4531, '芎': 4532, '犊': 4533, '乙': 4534, '轮': 4535, '呗': 4536, '邃': 4537, '绣': 4538, '金': 4539, '粗': 4540, '绒': 4541, '摹': 4542, '环': 4543, '盃': 4544, '锢': 4545, '冇': 4546, '嘲': 4547, '绑': 4548, '蛔': 4549, '你': 4550, '纶': 4551, '住': 4552, '迈': 4553, '翏': 4554, '桀': 4555, '呃': 4556, '呤': 4557, '臂': 4558, '盼': 4559, '咖': 4560, '签': 4561, '谍': 4562, '鱼': 4563, '目': 4564, '陌': 4565, '笨': 4566, ',': 4567, '忌': 4568, '滦': 4569, '弊': 4570, '盘': 4571, '传': 4572, '珠': 4573, '愈': 4574, '痤': 4575, '璨': 4576, '寄': 4577, '蚜': 4578, '狼': 4579, '屡': 4580, '桠': 4581, '春': 4582, '伉': 4583, '恿': 4584, '甙': 4585, '侈': 4586, '郴': 4587, 'F': 4588, '荷': 4589, '鹤': 4590, '彦': 4591, '赵': 4592, '秽': 4593, '遇': 4594, '舱': 4595, '＇': 4596, '舰': 4597, '酽': 4598, '才': 4599, '圩': 4600, '浚': 4601, '臼': 4602, '闺': 4603, '更': 4604, '驯': 4605, '浈': 4606, '蝗': 4607, '邡': 4608, '凫': 4609, '息': 4610, '须': 4611, '佩': 4612, '潭': 4613, '娣': 4614, '卉': 4615, '颇': 4616, '傧': 4617, '楂': 4618, '截': 4619, '掴': 4620, '肮': 4621, '谗': 4622, '﹕': 4623, '梵': 4624, '园': 4625, '酬': 4626, '侑': 4627, '烃': 4628, '轿': 4629, '镭': 4630, '算': 4631, '耒': 4632, '仰': 4633, '其': 4634, '歙': 4635, '递': 4636, '腭': 4637, '鸡': 4638, '着': 4639, '泉': 4640, '《': 4641, '憔': 4642, '殿': 4643, '兽': 4644, '嘎': 4645, '邵': 4646, '锏': 4647, '播': 4648, '避': 4649, '缈': 4650, '灸': 4651, '合': 4652, '栽': 4653, '定': 4654, '亿': 4655, '三': 4656, '戴': 4657, '稀': 4658, '﹒': 4659, '蓓': 4660, '{': 4661, '搪': 4662, '磋': 4663, '言': 4664, '鼠': 4665, '麂': 4666, '饮': 4667, '袓': 4668, '飞': 4669, '邮': 4670, '鏖': 4671, '酮': 4672, '粥': 4673, '也': 4674, '矸': 4675, '憎': 4676, '喷': 4677, '缷': 4678, '解': 4679, '倩': 4680, '腺': 4681, '遣': 4682, '啥': 4683, '越': 4684, '束': 4685, '療': 4686, '麟': 4687, '娲': 4688, '』': 4689, '荫': 4690, '菁': 4691, '古': 4692, '熨': 4693, '于': 4694, '勺': 4695, '☆': 4696, '塬': 4697, '躯': 4698, '橇': 4699, '押': 4700, '所': 4701, '窦': 4702, '恙': 4703, '昨': 4704, '握': 4705, '墙': 4706, '炉': 4707, '說': 4708, '直': 4709, '咎': 4710, '守': 4711, '禁': 4712, '窣': 4713, '跻': 4714, '擢': 4715, '杸': 4716, '码': 4717, '兴': 4718, '隘': 4719, '词': 4720, '商': 4721, '此': 4722, '佛': 4723, '爵': 4724, '瘾': 4725, '调': 4726, '划': 4727, '控': 4728, '歌': 4729, '怖': 4730, '笋': 4731, '眨': 4732, '韦': 4733, '坛': 4734, '堰': 4735, '员': 4736, '猜': 4737, '尔': 4738, '冠': 4739, '荔': 4740, '弋': 4741, '蕉': 4742, '阵': 4743, '幢': 4744, '忑': 4745, '幸': 4746, '济': 4747, '款': 4748, '铥': 4749, '嘅': 4750, '秋': 4751, '己': 4752, '琉': 4753, '降': 4754, '尸': 4755, '前': 4756, '缝': 4757, 'ä': 4758, '疚': 4759, '女': 4760, '苻': 4761, '蜇': 4762, '嗣': 4763, '庸': 4764, '葚': 4765, '钼': 4766, '氦': 4767, '詹': 4768, '钉': 4769, '幼': 4770, '快': 4771, '某': 4772, '契': 4773, '滁': 4774, '坤': 4775, '统': 4776, '痹': 4777, '绥': 4778, '扇': 4779, '叼': 4780, '掬': 4781, '讥': 4782, '扎': 4783, '岷': 4784, '槎': 4785, '鹰': 4786, '宜': 4787, '豁': 4788, '窗': 4789, '唧': 4790, '铌': 4791, '霈': 4792, '唛': 4793, '俞': 4794, '兮': 4795, '云': 4796, '蚀': 4797, '时': 4798, '洼': 4799, '飘': 4800, '铃': 4801, '季': 4802, '拷': 4803, '倏': 4804, '谕': 4805, '品': 4806, '株': 4807, '弄': 4808, '了': 4809, '啊': 4810, '寻': 4811, '缒': 4812, '盹': 4813, '晴': 4814, '拓': 4815, '镁': 4816, '钥': 4817, '甘': 4818, '嗦': 4819, '施': 4820, '叵': 4821, '螃': 4822, '迆': 4823, '擂': 4824, '洲': 4825, '会': 4826, '竞': 4827, '依': 4828, '批': 4829, '粘': 4830, '钾': 4831, '刑': 4832, '痘': 4833, '糙': 4834, '哪': 4835, '至': 4836, '推': 4837, '墩': 4838, '芮': 4839, '溶': 4840, '讳': 4841, '刘': 4842, '筠': 4843, '讷': 4844, '陲': 4845, '孟': 4846, '偁': 4847, '涓': 4848, '课': 4849, '炬': 4850, '旎': 4851, '筒': 4852, '螟': 4853, '祠': 4854, '肜': 4855, '运': 4856, '潲': 4857, '块': 4858, '淫': 4859, '最': 4860, '租': 4861, '址': 4862, '鸟': 4863, '哼': 4864, '姣': 4865, '榫': 4866, '僚': 4867, '罂': 4868, '佔': 4869, '宙': 4870, '矽': 4871, '殴': 4872, '楚': 4873, '聿': 4874, '后': 4875, '惚': 4876, '委': 4877, '蜷': 4878, '隅': 4879, '诘': 4880, '废': 4881, '切': 4882, '嗖': 4883, '茨': 4884, '螅': 4885, '要': 4886, '圭': 4887, '置': 4888, '褒': 4889, '蔑': 4890, '弗': 4891, '努': 4892, '叉': 4893, '俯': 4894, '５': 4895, '粤': 4896, '揿': 4897, '瓷': 4898, '檐': 4899, '奁': 4900, '瑞': 4901, '棕': 4902, '挡': 4903, '瞋': 4904, '殊': 4905, '在': 4906, '栓': 4907, '）': 4908, '钛': 4909, '勉': 4910, '妻': 4911, '瞭': 4912, '愕': 4913, '镗': 4914, '雹': 4915, '剩': 4916, '络': 4917, '船': 4918, '汇': 4919, '涕': 4920, '嵴': 4921, '父': 4922, '痒': 4923, '瞎': 4924, '芘': 4925, '跛': 4926, '呐': 4927, '惜': 4928, '醒': 4929, '斜': 4930, '嚓': 4931, '嵊': 4932, '夷': 4933, '烧': 4934, '应': 4935, '悯': 4936, '崔': 4937, '缏': 4938, '仙': 4939, '输': 4940, '窜': 4941, '无': 4942, '祉': 4943, '藏': 4944, '罴': 4945, '獍': 4946, '泳': 4947, '诱': 4948, '郧': 4949, '犍': 4950, '送': 4951, '咋': 4952, '剪': 4953, '乏': 4954, '僮': 4955, '盱': 4956, '衡': 4957, '菩': 4958, '律': 4959, '迎': 4960, '邪': 4961, '鼹': 4962, '管': 4963, '箝': 4964, '杭': 4965, '凖': 4966, '然': 4967, '任': 4968, '「': 4969, '奇': 4970, '穆': 4971, '荟': 4972, '油': 4973, '喙': 4974, '捐': 4975, '潦': 4976, '辜': 4977, '宋': 4978, '羸': 4979, '仇': 4980, '力': 4981, '荽': 4982, '啉': 4983, '橡': 4984, '辋': 4985, '骥': 4986, '媚': 4987, '邺': 4988, '刹': 4989, '陶': 4990, '葛': 4991, '且': 4992, '理': 4993, '惑': 4994, '衙': 4995, '期': 4996, '破': 4997, 'í': 4998, '座': 4999, '町': 5000, '魄': 5001, '授': 5002, '桧': 5003, '梨': 5004, '侯': 5005, '烘': 5006, '句': 5007, '○': 5008, '艿': 5009, '瑛': 5010, '尽': 5011, '乘': 5012, '侗': 5013, '狡': 5014, '脸': 5015, '芯': 5016, '奚': 5017, '犯': 5018, '阁': 5019, '溱': 5020, '煊': 5021, '钮': 5022, '檬': 5023, '悚': 5024, '冕': 5025, '共': 5026, '频': 5027, '勒': 5028, '毛': 5029, '局': 5030, '麒': 5031, '难': 5032, '故': 5033, '瘦': 5034, '苇': 5035, '思': 5036, '礁': 5037, '仔': 5038, '脆': 5039, '哲': 5040, '柳': 5041, '脚': 5042, '遥': 5043, '从': 5044, '途': 5045, '辐': 5046, '妃': 5047, '畴': 5048, '艏': 5049, '汛': 5050, '赴': 5051, '黑': 5052, '秉': 5053, '敛': 5054, '鲇': 5055, '捂': 5056, '躲': 5057, '凸': 5058, '妮': 5059, '韧': 5060, '岳': 5061, '嫩': 5062, '企': 5063, '益': 5064, '镍': 5065, '逆': 5066, '谜': 5067, '黍': 5068, '艮': 5069, '腱': 5070, '侵': 5071, '烫': 5072, '树': 5073, '锦': 5074, '蚴': 5075, '睫': 5076, '帐': 5077, '＞': 5078, '嚎': 5079, '奋': 5080, '暴': 5081, '癖': 5082, '盛': 5083, '锂': 5084, '疫': 5085, '儋': 5086, '♪': 5087, '恶': 5088, '谓': 5089, '劣': 5090, '郫': 5091, 'H': 5092, '篷': 5093, '嗡': 5094, '赤': 5095, '喘': 5096, '朱': 5097, '馅': 5098, '缪': 5099, '瓠': 5100, '叫': 5101, '投': 5102, '砚': 5103, '洗': 5104, '择': 5105, '询': 5106, '徽': 5107, '软': 5108, '套': 5109, '栅': 5110, '祀': 5111, '异': 5112, '℃': 5113, '斗': 5114, '尘': 5115, '嘟': 5116, '绸': 5117, '桂': 5118, '蛐': 5119, '氮': 5120, '茬': 5121, '陈': 5122, '凋': 5123, '罐': 5124, '哈': 5125, '舀': 5126, '囚': 5127, '福': 5128, '谛': 5129, '俺': 5130, '纪': 5131, '占': 5132, '画': 5133, '宣': 5134, '薏': 5135, '吩': 5136, '娘': 5137, '蛳': 5138, '圬': 5139, '袋': 5140, '朝': 5141, '购': 5142, '肌': 5143, '得': 5144, '吗': 5145, '雪': 5146, '晖': 5147, '&': 5148, '吴': 5149, '罼': 5150, 'ꊴ': 5151, '佚': 5152, '洒': 5153, '鬼': 5154, '尝': 5155, '盅': 5156, '廊': 5157, '〈': 5158, '葱': 5159, '联': 5160, '酌': 5161, '驮': 5162, '梯': 5163, '贾': 5164, '岫': 5165, '鸦': 5166, '焰': 5167, '旁': 5168, '都': 5169, '登': 5170, '胎': 5171, '焕': 5172, '跖': 5173, '包': 5174, '蓟': 5175, '酣': 5176, '吵': 5177, '樊': 5178, '濡': 5179, '捕': 5180, '雨': 5181, '泊': 5182, '孩': 5183, '蚧': 5184, '蛲': 5185, '禑': 5186, '叱': 5187, '垃': 5188, '士': 5189, '健': 5190, '咽': 5191, '褓': 5192, '倌': 5193, '死': 5194, '皆': 5195, '孰': 5196, '蹒': 5197, '旭': 5198, '朕': 5199, '职': 5200, '脊': 5201, '摔': 5202, '谐': 5203, '炯': 5204, '腊': 5205, '躅': 5206, '腩': 5207, '斯': 5208, '咝': 5209, '厥': 5210, '捡': 5211, '蹋': 5212, '战': 5213, '碘': 5214, '痱': 5215, '馊': 5216, '6': 5217, '胁': 5218, '捏': 5219, '匠': 5220, '糯': 5221, '路': 5222, '澜': 5223, '椎': 5224, '捎': 5225, '髌': 5226, '鲊': 5227, '箔': 5228, '蚊': 5229, '聂': 5230, '弓': 5231, '剂': 5232, '谊': 5233, '旅': 5234, '半': 5235, '甑': 5236, '预': 5237, '翼': 5238, '辑': 5239, '儆': 5240, '魇': 5241, '撰': 5242, '蹊': 5243, '尼': 5244, '矍': 5245, '镖': 5246, '裨': 5247, '整': 5248, '玎': 5249, '台': 5250, '笈': 5251, '粟': 5252, '暮': 5253, '梢': 5254, '|': 5255, '瞌': 5256, '尧': 5257, '夏': 5258, '蘅': 5259, '焙': 5260, '察': 5261, '录': 5262, '铦': 5263, '岬': 5264, '泸': 5265, '摺': 5266, '雳': 5267, '耀': 5268, '坊': 5269, '质': 5270, '榉': 5271, '裙': 5272, '削': 5273, '埋': 5274, '膦': 5275, '婚': 5276, '筘': 5277, '秸': 5278, '佐': 5279, '镕': 5280, '》': 5281, '再': 5282, '荒': 5283, '隍': 5284, '兔': 5285, '川': 5286, '伐': 5287, 'a': 5288, '汩': 5289, '法': 5290, '料': 5291, '振': 5292, '嫦': 5293, '懂': 5294, '殇': 5295, '名': 5296, '髂': 5297, '烙': 5298, '掂': 5299, '征': 5300, '愎': 5301, '署': 5302, '轭': 5303, '优': 5304, '伊': 5305, '呆': 5306, '纤': 5307, '蹈': 5308, '嫌': 5309, '赒': 5310, '鞘': 5311, '陷': 5312, '汕': 5313, '匾': 5314, '粝': 5315, '箬': 5316, '节': 5317, '凳': 5318, '炜': 5319, '惦': 5320, '沌': 5321, '党': 5322, '维': 5323, '蟑': 5324, '垒': 5325, '穴': 5326, '绞': 5327, '畲': 5328, '谙': 5329, '蚝': 5330, '疡': 5331, '赎': 5332, '噗': 5333, '蒙': 5334, '徕': 5335, '伺': 5336, '暑': 5337, '搓': 5338, '报': 5339, '袁': 5340, '—': 5341, '挪': 5342, '~': 5343, '獗': 5344, '裘': 5345, '寇': 5346, '闭': 5347, '掸': 5348, '脱': 5349, '拂': 5350, '洞': 5351, '帧': 5352, '吊': 5353, '读': 5354, '司': 5355, '竟': 5356, '描': 5357, '元': 5358, '五': 5359, '漆': 5360, '砼': 5361, '匮': 5362, '脰': 5363, '琛': 5364, '髡': 5365, '韩': 5366, '漠': 5367, '行': 5368, '猪': 5369, '喎': 5370, '恻': 5371, '毒': 5372, '嗜': 5373, '愤': 5374, 'I': 5375, '捅': 5376, '能': 5377, '桨': 5378, '泌': 5379, '榈': 5380, '泵': 5381, '流': 5382, '莽': 5383, '酱': 5384, '招': 5385, '砺': 5386, '參': 5387, '级': 5388, '麸': 5389, '柞': 5390, '岖': 5391, '跋': 5392, '患': 5393, '沈': 5394, '意': 5395, '阑': 5396, '荚': 5397, '秒': 5398, '钯': 5399, '鹊': 5400, '辕': 5401, '峪': 5402, '探': 5403, '帕': 5404, '瓮': 5405, '遏': 5406, '翩': 5407, '撮': 5408, '逶': 5409, '据': 5410, '根': 5411, '蜍': 5412, '翕': 5413, '萨': 5414, '般': 5415, '岗': 5416, '惠': 5417, '御': 5418, '过': 5419, '耸': 5420, '讼': 5421, '照': 5422, '阻': 5423, '查': 5424, '翜': 5425, '呀': 5426, '肾': 5427, '篮': 5428, '酒': 5429, '麋': 5430, '汗': 5431, '佤': 5432, '棒': 5433, '皖': 5434, '隆': 5435, '旦': 5436, '泞': 5437, '氘': 5438, '茄': 5439, '音': 5440, '哝': 5441, '达': 5442, '基': 5443, '骗': 5444, '历': 5445, '裴': 5446, '柢': 5447, '翡': 5448, '偷': 5449, '菏': 5450, '虑': 5451, '绕': 5452, '牙': 5453, '來': 5454, '侪': 5455, '籽': 5456, '帚': 5457, '贽': 5458, '闯': 5459, '臓': 5460, '漩': 5461, '神': 5462, '违': 5463, '刻': 5464, '检': 5465, '钆': 5466, '丧': 5467, '锪': 5468, '制': 5469, '盯': 5470, '展': 5471, '稜': 5472, '踝': 5473, '腥': 5474, '妲': 5475}\n"
     ]
    }
   ],
   "source": [
    "print('英文數據:\\n', en_data[:10])\n",
    "print('\\n中文數據:\\n', chi_data[:10])\n",
    "\n",
    "#生成英文字典\n",
    "en_vocab = set(''.join(en_data))\n",
    "id2en = list(en_vocab)\n",
    "en2id = {c:i for i,c in enumerate(id2en)}\n",
    "print('\\n英文字典:\\n', en2id)\n",
    "#生成中文字典\n",
    "ch_vocab = set(''.join(chi_data))\n",
    "id2ch = list(ch_vocab)\n",
    "ch2id = {c:i for i,c in enumerate(id2ch)}\n",
    "\n",
    "print('\\n中文字典共計\\n:', ch2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "char: He calls the Green Book, his book of teachings, “the new gospel.\n",
      "index: [954, 70, 735, 239, 436, 204, 204, 665, 735, 2, 349, 70, 735, 905, 568, 70, 70, 499, 735, 579, 192, 192, 16, 322, 735, 349, 209, 665, 735, 467, 192, 192, 16, 735, 192, 919, 735, 2, 70, 436, 239, 349, 209, 499, 818, 665, 322, 735, 654, 2, 349, 70, 735, 499, 70, 564, 735, 818, 192, 665, 747, 70, 204, 734]\n"
     ]
    }
   ],
   "source": [
    "en_num_data = [[en2id[en] for en in line ] for line in en_data]\n",
    "ch_num_data = [[ch2id[ch] for ch in line] for line in chi_data]\n",
    "de_num_data = [[ch2id[ch] for ch in line][1:] for line in chi_data]\n",
    "\n",
    "print('char:', en_data[1])\n",
    "print('index:', en_num_data[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000\n"
     ]
    }
   ],
   "source": [
    "print(len(en_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max encoder length: 361\n",
      "max decoder length: 198\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 80.8 GiB for an array with shape (60000, 361, 1001) and data type float32",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-bcf1fb44dcfa>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;31m# 將數據進行onehot處理\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[0mencoder_input_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0men_num_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_encoder_seq_length\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0men2id\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[0mdecoder_input_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mch_num_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_decoder_seq_length\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mch2id\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'float32'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[0mdecoder_target_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mch_num_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_decoder_seq_length\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mch2id\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'float32'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 80.8 GiB for an array with shape (60000, 361, 1001) and data type float32"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 獲取輸入輸出端的最大長度\n",
    "max_encoder_seq_length = max([len(txt) for txt in en_num_data])\n",
    "max_decoder_seq_length = max([len(txt) for txt in ch_num_data])\n",
    "print('max encoder length:', max_encoder_seq_length)\n",
    "print('max decoder length:', max_decoder_seq_length)\n",
    "\n",
    "# 將數據進行onehot處理\n",
    "encoder_input_data = np.zeros((len(en_num_data), max_encoder_seq_length, len(en2id)), dtype=np.float32)\n",
    "decoder_input_data = np.zeros((len(ch_num_data), max_decoder_seq_length, len(ch2id)), dtype='float32')\n",
    "decoder_target_data = np.zeros((len(ch_num_data), max_decoder_seq_length, len(ch2id)), dtype='float32')\n",
    "\n",
    "for i in range(len(ch_num_data)):\n",
    "    for t, j in enumerate(en_num_data[i]):\n",
    "        encoder_input_data[i, t, j] = 1.\n",
    "    for t, j in enumerate(ch_num_data[i]):\n",
    "        decoder_input_data[i, t, j] = 1.\n",
    "    for t, j in enumerate(de_num_data[i]):\n",
    "        decoder_target_data[i, t, j] = 1.\n",
    "\n",
    "print('index data:\\n', en_num_data[1])\n",
    "print('one hot data:\\n', encoder_input_data[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
